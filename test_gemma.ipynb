{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning of gemma-2b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL THE NECESSARY IMPORTS\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, HfArgumentParser, TrainingArguments\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "from peft import LoraConfig, TaskType, get_peft_model, get_peft_config\n",
    "\n",
    "# Filepath to embeddings\n",
    "fname = \"/mnt/mimic/data/HAIM/mimic_extras/embeddings.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the model\n",
    "\n",
    "Different versions, with huggingface LoRA-class or custom Adapter-module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5803ecd155274038b1fe78f3b59fb7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LoRA parameter efficient fine-tuning\n",
    "# Parameters are freezed and small submodules with low-rank matrices ar inserted at the target layers.\n",
    "# initialization of model\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\", quantization_config=quantization_config,attn_implementation=\"sdpa\")\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "gemma = get_peft_model(gemma, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,805,824 || all params: 2,515,978,240 || trainable%: 0.3897420034920493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GemmaForCausalLM(\n",
       "      (model): GemmaModel(\n",
       "        (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-17): 18 x GemmaDecoderLayer(\n",
       "            (self_attn): GemmaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): GemmaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): GemmaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=16384, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=16384, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=16384, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): GELUActivation()\n",
       "            )\n",
       "            (input_layernorm): GemmaRMSNorm()\n",
       "            (post_attention_layernorm): GemmaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): GemmaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model-structure and trainable parameters (this can be tuned by hyperparameters)\n",
    "gemma.print_trainable_parameters()\n",
    "gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapter NN for parameter efficient fine-tuning\n",
    "# Adapters (bottleneck feed-forward networks) are added as modules to the layers of the model\n",
    "# adapting attention projections and MLP projections while freezing original model parameters\n",
    "\n",
    "class Adapter(nn.Module):\n",
    "    def __init__(self, size = 6, model_dim = 2048):\n",
    "        super().__init__()\n",
    "        self.adapter_block = nn.Sequential(\n",
    "            nn.Linear(model_dim, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, model_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output = self.adapter_block(x)\n",
    "        adapter_out = output + x\n",
    "\n",
    "        return adapter_out\n",
    "\n",
    "\n",
    "class Adaptered(nn.Module):\n",
    "    def __init__(self, orig_layer):\n",
    "        super().__init__()\n",
    "        self.orig_layer = orig_layer\n",
    "        self.adapter = Adapter()\n",
    "\n",
    "    def forward(self, *x):\n",
    "        orig_out = self.orig_layer(*x)\n",
    "        output = (self.adapter.forward(orig_out[0].unsqueeze(0))[0],)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "class model_with_adapter(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\", quantization_config=self.quantization_config,attn_implementation=\"sdpa\")\n",
    "        # Freeze the original model parameters\n",
    "        for params in self.model.parameters():\n",
    "            params.requires_grad = False\n",
    "        # Embed adapter layers into the transformer blocks \n",
    "        for i, gemma_layer in enumerate(self.model.model.layers):\n",
    "            gemma_layer.self_attn.q_proj = Adaptered(gemma_layer.self_attn.q_proj)\n",
    "            gemma_layer.self_attn.k_proj = Adaptered(gemma_layer.self_attn.k_proj)\n",
    "            gemma_layer.self_attn.v_proj = Adaptered(gemma_layer.self_attn.v_proj)\n",
    "            gemma_layer.self_attn.o_proj = Adaptered(gemma_layer.self_attn.o_proj)\n",
    "    \n",
    "            gemma_layer.mlp.gate_proj = Adaptered(gemma_layer.mlp.gate_proj)\n",
    "            gemma_layer.mlp.up_proj = Adaptered(gemma_layer.mlp.up_proj)\n",
    "            gemma_layer.mlp.down_proj = Adaptered(gemma_layer.mlp.down_proj)\n",
    "\n",
    "    def get_model(self):\n",
    "\n",
    "        return self.model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom get_parameters function\n",
    "def get_parameters(model):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_percentage = (trainable_params / total_params) * 100\n",
    "\n",
    "    trainable_params_str = \"{:,}\".format(trainable_params)\n",
    "    total_params_str = \"{:,}\".format(total_params)\n",
    "\n",
    "    print(f\"trainable params: {trainable_params_str} || all params: {total_params_str} || trainable%: {trainable_percentage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0198f130175d4f38a30e07dbba33415c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,355,380 || all params: 1,518,623,476 || trainable%: 0.2209487771674616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model_with_adapter(\n",
       "  (model): GemmaForCausalLM(\n",
       "    (model): GemmaModel(\n",
       "      (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-17): 18 x GemmaDecoderLayer(\n",
       "          (self_attn): GemmaSdpaAttention(\n",
       "            (q_proj): Adaptered(\n",
       "              (orig_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (adapter): Adapter(\n",
       "                (adapter_block): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=6, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=6, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (k_proj): Adaptered(\n",
       "              (orig_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (adapter): Adapter(\n",
       "                (adapter_block): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=6, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=6, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (v_proj): Adaptered(\n",
       "              (orig_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (adapter): Adapter(\n",
       "                (adapter_block): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=6, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=6, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (o_proj): Adaptered(\n",
       "              (orig_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (adapter): Adapter(\n",
       "                (adapter_block): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=6, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=6, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (rotary_emb): GemmaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): GemmaMLP(\n",
       "            (gate_proj): Adaptered(\n",
       "              (orig_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "              (adapter): Adapter(\n",
       "                (adapter_block): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=6, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=6, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (up_proj): Adaptered(\n",
       "              (orig_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "              (adapter): Adapter(\n",
       "                (adapter_block): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=6, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=6, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (down_proj): Adaptered(\n",
       "              (orig_layer): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
       "              (adapter): Adapter(\n",
       "                (adapter_block): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=6, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=6, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (act_fn): GELUActivation()\n",
       "          )\n",
       "          (input_layernorm): GemmaRMSNorm()\n",
       "          (post_attention_layernorm): GemmaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): GemmaRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialization of adapter-model.\n",
    "# \n",
    "gemma = model_with_adapter().to('cuda')\n",
    "\n",
    "# Model-structure and trainable parameters (this can be tuned by hyperparameters)\n",
    "get_parameters(gemma)\n",
    "gemma\n",
    "\n",
    "# OBS A lot less params, not sure why.. (maybe cause of degeneration? or mistake, need to look into)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projection module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 1024\n",
    "projection_size = 6\n",
    "\n",
    "class ProjectionNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ProjectionNN, self).__init__()\n",
    "\n",
    "        # Architecture\n",
    "        self.fc1 = nn.Linear(embedding_size, 128).cuda()\n",
    "        self.relu = nn.ReLU().cuda()\n",
    "        self.fc2 = nn.Linear(128, 2048 * projection_size).cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(-1,6,2048)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching and preprocessing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_723060/4265014453.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_death_small48['y'] = 1\n",
      "/tmp/ipykernel_723060/4265014453.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_alive_big48['y'] = 0\n",
      "/tmp/ipykernel_723060/4265014453.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_death_big48['y'] = 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(fname)\n",
    "df_death_small48 = df[((df['img_length_of_stay'] < 48) & (df['death_status'] == 1))]\n",
    "df_alive_big48 = df[((df['img_length_of_stay'] >= 48) & (df['death_status'] == 0))]\n",
    "df_death_big48 = df[((df['img_length_of_stay'] >= 48) & (df['death_status'] == 1))]\n",
    "\n",
    "df_death_small48['y'] = 1\n",
    "df_alive_big48['y'] = 0\n",
    "df_death_big48['y'] = 0\n",
    "df = pd.concat([df_death_small48, df_alive_big48, df_death_big48], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     haim_id      vd_0      vd_1      vd_2      vd_3      vd_4      vd_5  \\\n",
      "256     6557  0.005299  0.082119  0.274407  0.017487  0.255308  0.003707   \n",
      "259     6557  0.000000  0.079306  0.381579  0.015250  0.402685  0.011122   \n",
      "267     6558  0.005299  0.082119  0.274407  0.017487  0.255308  0.003707   \n",
      "270     6558  0.000000  0.079306  0.381579  0.015250  0.402685  0.011122   \n",
      "319     6581  0.002288  0.078941  0.088397  0.017775  0.071482  0.006970   \n",
      "\n",
      "         vd_6      vd_7      vd_8  ...   vd_1015   vd_1016   vd_1017  \\\n",
      "256  0.137267  0.024046  0.145395  ...  0.008003  0.013876  0.005360   \n",
      "259  0.125938  0.033254  0.227433  ...  0.042140  0.036560  0.006585   \n",
      "267  0.137267  0.024046  0.145395  ...  0.008003  0.013876  0.005360   \n",
      "270  0.125938  0.033254  0.227433  ...  0.042140  0.036560  0.006585   \n",
      "319  0.223354  0.045017  0.056177  ...  0.004973  0.000343  0.000000   \n",
      "\n",
      "      vd_1018   vd_1019   vd_1020   vd_1021   vd_1022   vd_1023  y  \n",
      "256  0.039292  0.029467  0.003972  0.000203  0.024739  0.005796  1  \n",
      "259  0.042200  0.029051  0.006776  0.000000  0.022821  0.011584  1  \n",
      "267  0.039292  0.029467  0.003972  0.000203  0.024739  0.005796  1  \n",
      "270  0.042200  0.029051  0.006776  0.000000  0.022821  0.011584  1  \n",
      "319  0.005877  0.005696  0.000000  0.000000  0.013369  0.168811  1  \n",
      "\n",
      "[5 rows x 1026 columns]\n"
     ]
    }
   ],
   "source": [
    "vd_cols = df.filter(regex='^vd_')\n",
    "y_col = df[['y']]\n",
    "haim_col = df[['haim_id']]\n",
    "df = pd.concat([haim_col, vd_cols, y_col], axis=1)\n",
    "\n",
    "pkl_list = df['haim_id'].unique().tolist()\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_split(df, pkl_list, test_size=0.3, validation_size=0.1, random_state=None):\n",
    "    # Split into training and test sets\n",
    "    train_set, test_set = train_test_split(pkl_list, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Further split the training set into training and validation sets\n",
    "    train_set, validation_set = train_test_split(train_set, test_size=validation_size, random_state=random_state)\n",
    "\n",
    "    train_idx = df[df['haim_id'].isin(train_set)]['haim_id'].tolist()\n",
    "    validation_idx = df[df['haim_id'].isin(validation_set)]['haim_id'].tolist()\n",
    "    test_idx = df[df['haim_id'].isin(test_set)]['haim_id'].tolist()\n",
    "\n",
    "    x_train = df[df['haim_id'].isin(train_idx)].drop(['haim_id', 'y'], axis=1).values\n",
    "    x_validation = df[df['haim_id'].isin(validation_idx)].drop(['haim_id', 'y'], axis=1).values\n",
    "    x_test = df[df['haim_id'].isin(test_idx)].drop(['haim_id', 'y'], axis=1).values\n",
    "\n",
    "    y_train = df[df['haim_id'].isin(train_idx)]['y'].values\n",
    "    y_validation = df[df['haim_id'].isin(validation_idx)]['y'].values\n",
    "    y_test = df[df['haim_id'].isin(test_idx)]['y'].values\n",
    "\n",
    "    return x_train, x_validation, x_test, y_train, y_validation, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(df, pkl_list, test_size=0.3, validation_size=0.1, random_state=None):\n",
    "\n",
    "    types = ['vd_', 'vp_', 'vmd_', 'ts_ce_', 'ts_le_', 'ts_pe_', 'n_rad_']\n",
    "\n",
    "    x_train = {}\n",
    "    x_validation = {}\n",
    "    x_test = {}\n",
    "\n",
    "    # Split into training and test sets\n",
    "    train_set, test_set = train_test_split(pkl_list, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    remaining_data_size = 1.0 - test_size\n",
    "\n",
    "    validation_size = validation_size*remaining_data_size\n",
    "\n",
    "    # Further split the training set into training and validation sets\n",
    "    train_set, validation_set = train_test_split(train_set, test_size=validation_size, random_state=random_state)\n",
    "\n",
    "    train_idx = df[df['haim_id'].isin(train_set)]['haim_id'].tolist()\n",
    "    validation_idx = df[df['haim_id'].isin(validation_set)]['haim_id'].tolist()\n",
    "    test_idx = df[df['haim_id'].isin(test_set)]['haim_id'].tolist()\n",
    "\n",
    "    for t in types:\n",
    "        \n",
    "        x_train[t] = df[df['haim_id'].isin(train_idx)].filter(regex='^'+t).values\n",
    "        x_validation[t] = df[df['haim_id'].isin(validation_idx)].filter(regex='^'+t).values\n",
    "        x_test[t] = df[df['haim_id'].isin(test_idx)].filter(regex='^'+t).values\n",
    "\n",
    "    y_train = df[df['haim_id'].isin(train_idx)]['y'].values\n",
    "    y_train = [' ###ANSWER: No' if value == 0 else ' ###ANSWER: Yes' for value in y_train]\n",
    "    y_validation = df[df['haim_id'].isin(validation_idx)]['y'].values\n",
    "    y_validation = [' ###ANSWER: No' if value == 0 else ' ###ANSWER: Yes' for value in y_validation]\n",
    "    y_test = df[df['haim_id'].isin(test_idx)]['y'].values\n",
    "    y_test = [' ###ANSWER: No' if value == 0 else ' ###ANSWER: Yes' for value in y_test]\n",
    "\n",
    "    return x_train, x_validation, x_test, y_train, y_validation, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, x_test, y_train, y_val, y_test, = data_split(df, pkl_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSplit():\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.types = ['vd_', 'vp_', 'vmd_', 'ts_ce_', 'ts_le_', 'ts_pe_', 'n_rad_']\n",
    "        self.partition = None\n",
    "\n",
    "    def partitiondata(self, partition):\n",
    "        self.pkl_list = []\n",
    "        if partition == 'mortality':\n",
    "            condition_death_small48 = (self.df['img_length_of_stay'] < 48) & (self.df['death_status'] == 1)\n",
    "\n",
    "            y = [0]*len(self.df)\n",
    "            for i, condition in enumerate(condition_death_small48):\n",
    "                if condition:\n",
    "                    y[i] = 1\n",
    "\n",
    "        if partition == 'los':\n",
    "            condition_alive_small48 = self.df[((self.df['img_length_of_stay'] < 48) & (self.df['death_status'] == 0))]\n",
    "\n",
    "            y = [0]*len(self.df)\n",
    "            for i, condition in enumerate(condition_alive_small48):\n",
    "                if condition:\n",
    "                    y[i] = 1\n",
    "\n",
    "        self.df['y'] = y\n",
    "\n",
    "    def get_data(self, partition, test_size=0.3, validation_size=0.1, random_state=None):\n",
    "\n",
    "        self.partition = partition\n",
    "\n",
    "        self.partitiondata(partition)\n",
    "        pkl_list = self.df['haim_id'].unique().tolist()\n",
    "\n",
    "        # Split into training and test sets\n",
    "        train_set, test_set = train_test_split(pkl_list, test_size=test_size, random_state=random_state)\n",
    "\n",
    "        remaining_data_size = 1.0 - test_size\n",
    "        validation_size = validation_size*remaining_data_size\n",
    "\n",
    "        # Further split the training set into training and validation sets\n",
    "        train_set, validation_set = train_test_split(train_set, test_size=validation_size, random_state=random_state)\n",
    "\n",
    "        train_idx = self.df[self.df['haim_id'].isin(train_set)]['haim_id'].tolist()\n",
    "        validation_idx = self.df[self.df['haim_id'].isin(validation_set)]['haim_id'].tolist()\n",
    "        test_idx = self.df[self.df['haim_id'].isin(test_set)]['haim_id'].tolist()\n",
    "\n",
    "        self.x_train = {t: self.df[self.df['haim_id'].isin(train_idx)].filter(regex='^'+t).values for t in self.types}\n",
    "        self.x_validation = {t: self.df[self.df['haim_id'].isin(validation_idx)].filter(regex='^'+t).values for t in self.types}\n",
    "        self.x_test = {t: self.df[self.df['haim_id'].isin(test_idx)].filter(regex='^'+t).values for t in self.types}\n",
    "\n",
    "        self.y_train = df[df['haim_id'].isin(train_idx)]['y'].values\n",
    "        self.y_validation = df[df['haim_id'].isin(validation_idx)]['y'].values\n",
    "        self.y_test = df[df['haim_id'].isin(test_idx)]['y'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, embedding, labels):\n",
    "        self.labels = labels\n",
    "        self.embedding = embedding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        emb = self.embedding[idx]\n",
    "        sample = {\"Emb\": emb, \"Class\": label}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_batch for 0/1 labels\n",
    "\n",
    "def collate_batch(batch):\n",
    "     \n",
    "    emb_list, classes = [], []\n",
    "    for thing in batch:\n",
    "        #print(batch)\n",
    "        emb_list.append(thing['Emb'])\n",
    "        classes.append(thing['Class'])\n",
    "    text = torch.tensor(emb_list)\n",
    "    classes = torch.tensor(classes, dtype=torch.float16)\n",
    "    return text, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CustomDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m ProjectionNN()\n\u001b[0;32m----> 3\u001b[0m Trainset \u001b[38;5;241m=\u001b[39m \u001b[43mCustomDataset\u001b[49m(x_train\u001b[38;5;241m.\u001b[39mtolist(), y_train)\n\u001b[1;32m      4\u001b[0m Valset \u001b[38;5;241m=\u001b[39m CustomDataset(x_val, y_gen_val)\n\u001b[1;32m      5\u001b[0m Testset \u001b[38;5;241m=\u001b[39m CustomDataset(x_test, y_gen_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CustomDataset' is not defined"
     ]
    }
   ],
   "source": [
    "model = ProjectionNN()\n",
    "\n",
    "Trainset = CustomDataset(x_train.tolist(), y_train)\n",
    "Valset = CustomDataset(x_val, y_gen_val)\n",
    "Testset = CustomDataset(x_test, y_gen_test)\n",
    "\n",
    "Train_loader = DataLoader(Trainset, batch_size=1, collate_fn=collate_batch)\n",
    "Val_loader = DataLoader(Valset, batch_size=1, collate_fn=collate_batch)\n",
    "Test_loader = DataLoader(Testset, batch_size=1, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProjectionNN()\n",
    "\n",
    "Trainset = CustomDataset(x_train.tolist(), y_gen_train)\n",
    "Valset = CustomDataset(x_val, y_val)\n",
    "Testset = CustomDataset(x_test, y_test)\n",
    "\n",
    "Train_loader = DataLoader(Trainset, batch_size=1, collate_fn=collate_batch)\n",
    "Val_loader = DataLoader(Valset, batch_size=1, collate_fn=collate_batch)\n",
    "Test_loader = DataLoader(Testset, batch_size=1, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_output(emb, gemma):\n",
    "    outputs = gemma(inputs_embeds=emb)\n",
    "    noyes = [1294, 3276]\n",
    "    logits = outputs['logits']\n",
    "    logits = logits[:,-1,noyes]\n",
    "    return logits\n",
    "\n",
    "def output_to_label(logits):\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_token_id = torch.argmax(probs, dim=-1)\n",
    "    return predicted_token_id\n",
    "\n",
    "    \n",
    "def train_epoch(model, gemma, optimizer, loss_fn, train_loader, device):\n",
    "    # Train:\n",
    "    model.train()\n",
    "    train_loss_batches, train_acc_batches = [], []\n",
    "    num_batches = len(train_loader)\n",
    "    for batch_index, (x, y) in enumerate(train_loader, 1):\n",
    "        inputs, labels = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        emb = model(inputs)\n",
    "        logits = custom_output(emb.to(torch.float16), gemma)\n",
    "        \n",
    "        loss = loss_fn(logits, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_batches.append(loss.item())\n",
    "\n",
    "        hard_preds = output_to_label(logits)\n",
    "        acc_batch_avg = (hard_preds == labels).float().mean().item()\n",
    "        train_acc_batches.append(acc_batch_avg)\n",
    "\n",
    "    return model, train_loss_batches, train_acc_batches\n",
    "\n",
    "def validate(model, gemma, loss_fn, val_loader, device, word_embs):\n",
    "    val_loss_cum = 0\n",
    "    val_acc_cum = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (x, y) in enumerate(val_loader, 1):\n",
    "            inputs, labels = x.to(device), y.to(device)\n",
    "            emb = model.forward(inputs)\n",
    "            concatted = torch.cat((word_embs,emb), dim=1).to(torch.float16)\n",
    "            logits = custom_output(concatted, gemma)\n",
    "\n",
    "            batch_loss = loss_fn(logits, labels.float())\n",
    "            val_loss_cum += batch_loss.item()\n",
    "            hard_preds = output_to_label(logits)\n",
    "            acc_batch_avg = (hard_preds == labels).float().mean().item()\n",
    "            val_acc_cum += acc_batch_avg\n",
    "    return val_loss_cum/len(val_loader), val_acc_cum/len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, gemma, optimizer, loss_fn, train_loader, val_loader, num_epochs):\n",
    "    print(\"Starting training\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model, train_loss, train_acc = train_epoch(model, gemma,\n",
    "                                                   optimizer,\n",
    "                                                   loss_fn,\n",
    "                                                   train_loader,\n",
    "                                                   device)\n",
    "        val_loss, val_acc = validate(model, loss_fn, val_loader, device)\n",
    "        print(f\"Epoch {epoch}/{num_epochs}: \"\n",
    "              f\"Train loss: {sum(train_loss)/len(train_loss):.3f}, \"\n",
    "              f\"Train acc.: {sum(train_acc)/len(train_acc):.3f}, \"\n",
    "              f\"Val. loss: {val_loss:.3f}, \"\n",
    "              f\"Val. acc.: {val_acc:.3f}\")\n",
    "        train_losses.extend(train_loss)\n",
    "        train_accs.extend(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "    return model, train_losses, train_accs, val_losses, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "cuda\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m      2\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgemma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVal_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(model, gemma, optimizer, loss_fn, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[1;32m      9\u001b[0m train_losses, train_accs, val_losses, val_accs \u001b[38;5;241m=\u001b[39m [], [], [], []\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m     model, train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgemma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m validate(model, loss_fn, val_loader, device)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(train_loss)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loss)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain acc.: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(train_acc)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_acc)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal. loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal. acc.: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[40], line 40\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, gemma, optimizer, loss_fn, train_loader, device)\u001b[0m\n\u001b[1;32m     37\u001b[0m targets_masked \u001b[38;5;241m=\u001b[39m padding_tensor[mask]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     39\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(logits_masked\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m), targets_masked)\n\u001b[0;32m---> 40\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     42\u001b[0m train_loss_batches\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/home/edgelab/miniconda3/envs/gemma/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m/home/edgelab/miniconda3/envs/gemma/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "training_loop(model, gemma, optimizer, loss_fn, Train_loader, Val_loader, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 - Inputs: tensor([[0.0023, 0.0789, 0.0884,  ..., 0.0000, 0.0134, 0.1688]]), Labels: tensor([[     2,   6176, 221049, 235292,  11396,    736,   7679,   4692,   5543,\n",
      "           1178, 235248, 235310, 235321,    531, 235336,  43774,  11189,  44702,\n",
      "         235292, 235248]]), Instructions tensor([[     2,  43774,  72601, 235292,   6287]])\n",
      "Batch 2 - Inputs: tensor([[0.0104, 0.0867, 0.1055,  ..., 0.0014, 0.0057, 0.0628]]), Labels: tensor([[     2,   6176, 221049, 235292,  11396,    736,   7679,   4692,   5543,\n",
      "           1178, 235248, 235310, 235321,    531, 235336,  43774,  11189,  44702,\n",
      "         235292, 235248]]), Instructions tensor([[     2,  43774,  72601, 235292,   6287]])\n",
      "Batch 3 - Inputs: tensor([[0.0067, 0.0579, 0.1242,  ..., 0.0031, 0.0000, 0.0049]]), Labels: tensor([[     2,   6176, 221049, 235292,  11396,    736,   7679,   4692,   5543,\n",
      "           1178, 235248, 235310, 235321,    531, 235336,  43774,  11189,  44702,\n",
      "         235292, 235248]]), Instructions tensor([[     2,  43774,  72601, 235292,   6287]])\n",
      "Batch 4 - Inputs: tensor([[0.0024, 0.0508, 0.2768,  ..., 0.0048, 0.0049, 0.0175]]), Labels: tensor([[     2,   6176, 221049, 235292,  11396,    736,   7679,   4692,   5543,\n",
      "           1178, 235248, 235310, 235321,    531, 235336,  43774,  11189,  44702,\n",
      "         235292, 235248]]), Instructions tensor([[     2,  43774,  72601, 235292,   6287]])\n"
     ]
    }
   ],
   "source": [
    "for batch_index, (x, y, z) in enumerate(Train_loader, 1):\n",
    "    print(f\"Batch {batch_index} - Inputs: {x}, Labels: {y}, Instructions {z}\")\n",
    "\n",
    "    if batch_index > 3:\n",
    "        break  # Break the loop after printing the first three batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for generative\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, embedding, labels):\n",
    "        self.labels = labels\n",
    "        self.embedding = embedding\n",
    "        self.instruction = \"###INSTRUCTION: Did this patient stay longer than 48 h? ###MODALITY: \"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        emb = self.embedding[idx]\n",
    "        inst = self.instruction\n",
    "        sample = {\"Emb\": emb, \"Class\": label, 'Inst': inst}\n",
    "        #print(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_batch for generative\n",
    "\n",
    "def collate_batch(batch):\n",
    "     \n",
    "    emb_list, classes, instructions = [], [], []\n",
    "    for thing in batch:\n",
    "        emb_list.append(thing['Emb'])\n",
    "        classes.append(tokenizer(thing['Class'], return_tensors=\"pt\"))\n",
    "        instructions.append(tokenizer(thing['Inst'], return_tensors=\"pt\"))\n",
    "    text = torch.tensor(emb_list)\n",
    "    classes = torch.cat([item['input_ids'] for item in classes], dim=0)\n",
    "    instructions = torch.cat([item['input_ids'] for item in instructions], dim=0)\n",
    "    return text, instructions, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST APPROACH GEN TRYOUT\n",
    "\n",
    "def train_epoch(model, gemma, optimizer, loss_fn, train_loader, device):\n",
    "    # Train:\n",
    "    model.train()\n",
    "    train_loss_batches, train_acc_batches = [], []\n",
    "    num_batches = len(train_loader)\n",
    "    embedding_matrix = gemma.get_input_embeddings().weight\n",
    "    for batch_index, (mod, inst, label) in enumerate(train_loader, 1):\n",
    "        mod_embeddings = model(mod.to(device))\n",
    "        print(mod_embeddings.device)\n",
    "        inst_list = [embedding_matrix[token_id].to(device) for token_id in inst.to(dtype=torch.long)]\n",
    "        label_list = [embedding_matrix[token_id].to(device) for token_id in label.to(dtype=torch.long)]\n",
    "        inst_embeddings = torch.stack(inst_list)\n",
    "        label_embeddings = torch.stack(label_list)\n",
    "        print(label)\n",
    "        \n",
    "        mod_embeddings = mod_embeddings.to(device)\n",
    "        #print(mod_embeddings.shape)\n",
    "        #print(inst_embeddings.shape)\n",
    "        #print(label_embeddings.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        conc_emb = torch.cat([inst_embeddings.to(dtype=torch.float16).to(device), mod_embeddings.to(device), label_embeddings.to(dtype=torch.float16).to(device)], dim=1).to(device)\n",
    "        target = label.to(dtype=torch.float16)\n",
    "        print(target.shape)\n",
    "        print(conc_emb.shape)\n",
    "        output = gemma(inputs_embeds=conc_emb.to(dtype=torch.float16))\n",
    "        print('forward: ', output['logits'].shape)\n",
    "        probabilities = F.softmax(output['logits'], dim=-1)\n",
    "        print('probabilities: ', probabilities.shape)\n",
    "        predicted_ids = torch.argmax(probabilities, dim=-1)\n",
    "        print('predicted ids: ', predicted_ids.shape)\n",
    "\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_batches.append(loss.item())\n",
    "\n",
    "        hard_preds = output_to_label(logits)\n",
    "        acc_batch_avg = (hard_preds == labels).float().mean().item()\n",
    "        train_acc_batches.append(acc_batch_avg)\n",
    "\n",
    "    return model, train_loss_batches, train_acc_batches\n",
    "\n",
    "def validate(model, gemma, loss_fn, val_loader, device, word_embs):\n",
    "    val_loss_cum = 0\n",
    "    val_acc_cum = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (x, y) in enumerate(val_loader, 1):\n",
    "            inputs, labels = x.to(device), y.to(device)\n",
    "            emb = model.forward(inputs)\n",
    "            concatted = torch.cat((word_embs,emb), dim=1).to(torch.float16)\n",
    "            logits = custom_output(concatted, gemma)\n",
    "\n",
    "            batch_loss = loss_fn(logits, labels.float())\n",
    "            val_loss_cum += batch_loss.item()\n",
    "            hard_preds = output_to_label(logits)\n",
    "            acc_batch_avg = (hard_preds == labels).float().mean().item()\n",
    "            val_acc_cum += acc_batch_avg\n",
    "    return val_loss_cum/len(val_loader), val_acc_cum/len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def left_padding(concatenated_emb, label, device):\n",
    "    prompt_length = concatenated_emb.size(1) - label[0].size(0)\n",
    "    padded_labels= torch.cat([torch.full((prompt_length,), -100).to(device), label[0].to(device)]).to(device)\n",
    "    return padded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masking(logits, padded_target):\n",
    "    print(padded_target)\n",
    "    mask = (padded_target != -100)\n",
    "    logits_masked = logits[:, mask, :]\n",
    "    targets_masked = padded_target[mask].view(-1)\n",
    "    return logits_masked, targets_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECOND APPROACH GEN TRYOUT\n",
    "\n",
    "def output_to_label(logits):\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_token_id = torch.argmax(probs, dim=-1)\n",
    "    return predicted_token_id\n",
    "\n",
    "def train_epoch(model, gemma, optimizer, loss_fn, train_loader, device):\n",
    "    # Train:\n",
    "    gemma.train()\n",
    "    train_loss_batches, train_acc_batches = [], []\n",
    "    num_batches = len(train_loader)\n",
    "    embedding_matrix = gemma.get_input_embeddings().weight\n",
    "    for batch_index, (mod, inst, label) in enumerate(train_loader, 1):\n",
    "        mod_embeddings = model(mod.to(device))\n",
    "        inst_list = [embedding_matrix[token_id].to(device) for token_id in inst.to(dtype=torch.long)]\n",
    "        label_list = [embedding_matrix[token_id].to(device) for token_id in label.to(dtype=torch.long)]\n",
    "        inst_embeddings = torch.stack(inst_list)\n",
    "        label_embeddings = torch.stack(label_list)\n",
    "        \n",
    "        mod_embeddings = mod_embeddings.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        conc_emb = torch.cat([inst_embeddings.to(dtype=torch.float16).to(device), mod_embeddings.to(device), label_embeddings.to(dtype=torch.float16).to(device)], dim=1).to(device)\n",
    "        prompt_length = conc_emb.size(1) - label[0].size(0)\n",
    "\n",
    "        padding_tensor = torch.cat([torch.full((prompt_length,), -100).to('cuda'), label[0].to(device)]).to(device)\n",
    "\n",
    "\n",
    "        output = gemma(inputs_embeds=conc_emb.to(dtype=torch.float16), labels=padding_tensor)\n",
    "\n",
    "        logits = output['logits'] #torch.Size([1, 31, 256000])\n",
    "        mask = (padding_tensor != -100) #torch.Size([31])\n",
    "        logits_masked = logits[:, mask, :]\n",
    "\n",
    "        targets_masked = padding_tensor[mask].view(-1)\n",
    "\n",
    "        loss = loss_fn(logits_masked.squeeze(0), targets_masked)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_batches.append(loss.item())\n",
    "\n",
    "        hard_preds = output_to_label(logits_masked)\n",
    "        acc_batch_avg = (hard_preds == targets_masked).float().mean().item()\n",
    "        train_acc_batches.append(acc_batch_avg)\n",
    "\n",
    "    return model, train_loss_batches, train_acc_batches\n",
    "\n",
    "def validate(model, gemma, loss_fn, val_loader, device, word_embs):\n",
    "    val_loss_cum = 0\n",
    "    val_acc_cum = 0\n",
    "    gemma.eval()\n",
    "    embedding_matrix = gemma.get_input_embeddings().weight\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (mod, inst, label) in enumerate(train_loader, 1):\n",
    "            mod_embeddings = model(mod.to(device))\n",
    "            inst_list = [embedding_matrix[token_id].to(device) for token_id in inst.to(dtype=torch.long)]\n",
    "            label_list = [embedding_matrix[token_id].to(device) for token_id in label.to(dtype=torch.long)]\n",
    "            inst_embeddings = torch.stack(inst_list)\n",
    "            label_embeddings = torch.stack(label_list)\n",
    "        \n",
    "            mod_embeddings = mod_embeddings.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            conc_emb = torch.cat([inst_embeddings.to(dtype=torch.float16).to(device), mod_embeddings.to(device), label_embeddings.to(dtype=torch.float16).to(device)], dim=1).to(device)\n",
    "            prompt_length = conc_emb.size(1) - label[0].size(0)\n",
    "\n",
    "            padding_tensor = torch.cat([torch.full((prompt_length,), -100).to('cuda'), label[0].to(device)]).to(device)\n",
    "\n",
    "\n",
    "            output = gemma(inputs_embeds=conc_emb.to(dtype=torch.float16), labels=padding_tensor)\n",
    "\n",
    "            logits = output['logits'] #torch.Size([1, 31, 256000])\n",
    "            mask = (padding_tensor != -100) #torch.Size([31])\n",
    "            logits_masked = logits[:, mask, :]\n",
    "\n",
    "            targets_masked = padding_tensor[mask].view(-1)\n",
    "\n",
    "\n",
    "            loss = loss_fn(logits_masked.squeeze(0), targets_masked)\n",
    "            val_loss_cum += loss.item()\n",
    "            hard_preds = output_to_label(logits_masked)\n",
    "            acc_batch_avg = (hard_preds == targets_masked).float().mean().item()\n",
    "            val_acc_cum += acc_batch_avg\n",
    "    return val_loss_cum/len(val_loader), val_acc_cum/len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, gemma, optimizer, loss_fn, train_loader, val_loader, num_epochs):\n",
    "    print(\"Starting training\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "    gemma.to(device)\n",
    "    #for name, param in gemma.named_parameters():\n",
    "    #    print(f\"{name}: {param.device}\")\n",
    "    train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model, train_loss, train_acc = train_epoch(model, gemma,\n",
    "                                                   optimizer,\n",
    "                                                   loss_fn,\n",
    "                                                   train_loader,\n",
    "                                                   device)\n",
    "        val_loss, val_acc = validate(model, loss_fn, val_loader, device)\n",
    "        print(f\"Epoch {epoch}/{num_epochs}: \"\n",
    "              f\"Train loss: {sum(train_loss)/len(train_loss):.3f}, \"\n",
    "              f\"Train acc.: {sum(train_acc)/len(train_acc):.3f}, \"\n",
    "              f\"Val. loss: {val_loss:.3f}, \"\n",
    "              f\"Val. acc.: {val_acc:.3f}\")\n",
    "        train_losses.extend(train_loss)\n",
    "        train_accs.extend(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "    return model, train_losses, train_accs, val_losses, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 256000/256000 [00:01<00:00, 178866.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No:  [793, 956, 1294, 1307]\n",
      "Yes:  [3276, 3553, 6287, 7778]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nos = ['No', 'no', ' No', ' no', 'no ', 'No ']\n",
    "yes = ['Yes', 'yes', ' Yes', ' yes', 'yes ', 'Yes ']\n",
    "embedding_matrix = gemma.get_input_embeddings().weight\n",
    "print(len(embedding_matrix))\n",
    "\n",
    "\n",
    "no_labels = []\n",
    "yes_labels = []\n",
    "for n in tqdm(range(256000)):\n",
    "    t = tokenizer.decode(n)\n",
    "    if t in yes:\n",
    "        yes_labels.append(n)\n",
    "    if t in nos:\n",
    "        no_labels.append(n)\n",
    "\n",
    "print('No: ', no_labels)\n",
    "print('Yes: ', yes_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "21\n",
      "tensor([[229711,    714, 235292,    109,    692,   1707,    791,    575,    575,\n",
      "           5043, 235304,   3763,   3763, 235336,    109,  61278, 229711, 222975,\n",
      "           1692, 130983,  61278,    109,    109, 235292,    590, 235269]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' increa The:\\n\\n you help have in in expected3 hours hours?\\n\\nMathML increa WhencetabularBASELINEMathML\\n\\n\\n\\n: I,'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying out finetune of generative aspect of gemma\n",
    "\n",
    "embedding_matrix = gemma.get_input_embeddings().weight\n",
    "prompt = \"### INSTRUCTION: Did this patient stay longer than 48 h?\"\n",
    "lab = '### ANSWER: Yes'\n",
    "mod = torch.randn(1, 1024).to('cuda')\n",
    "prompt_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "lab_ids = tokenizer(lab, return_tensors=\"pt\").to(\"cuda\")\n",
    "print(len(lab_ids))\n",
    "#print(prompt_ids[\"input_ids\"].shape)\n",
    "prompt_embeddings = [embedding_matrix[token_id].to('cuda') for token_id in prompt_ids[\"input_ids\"][0]]\n",
    "#print(lab.shape)\n",
    "prompt_embeddings = torch.stack(prompt_embeddings)\n",
    "label_embeddings = [embedding_matrix[token_id].to('cuda') for token_id in lab_ids[\"input_ids\"][0]]\n",
    "label_embeddings = torch.stack(label_embeddings)\n",
    "\n",
    "p_mod = model(mod)\n",
    "\n",
    "conc_emb = torch.cat([prompt_embeddings.unsqueeze(0).to(dtype=torch.float16), p_mod.to('cuda').to(dtype=torch.float16), label_embeddings.unsqueeze(0).to(dtype=torch.float16)], dim=1)\n",
    "\n",
    "prompt_length = conc_emb.size(1) - lab_ids['input_ids'][0].size(0)\n",
    "print(prompt_length)\n",
    "\n",
    "padding_tensor = torch.cat([torch.full((prompt_length,), -100).to('cuda'), lab_ids['input_ids'][0]])\n",
    "\n",
    "#print(padding_tensor)\n",
    "#print(conc_emb.shape)\n",
    "conc_emb.to(dtype=torch.float16).to('cuda')\n",
    "output = gemma(inputs_embeds=conc_emb, labels=padding_tensor)\n",
    "#print(output)\n",
    "\n",
    "probabilities = F.softmax(output['logits'], dim=-1)\n",
    "predicted_ids = torch.argmax(probabilities, dim=-1)\n",
    "print(predicted_ids)\n",
    "\n",
    "tokenizer.decode(predicted_ids[0])\n",
    "#tokenizer.decode(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.2 ('gemma')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10601c873fb2576e1e1a48994b394f584387b3d54a28d8ac07023c991446672f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
