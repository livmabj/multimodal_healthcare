{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning of gemma-2b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL THE NECESSARY IMPORTS\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, HfArgumentParser, TrainingArguments\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "from peft import LoraConfig, TaskType, get_peft_model, get_peft_config\n",
    "\n",
    "# Filepath to embeddings\n",
    "fname = \"/mnt/mimic/data/HAIM/mimic_extras/embeddings.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the model\n",
    "\n",
    "Different versions, with huggingface LoRA-class or custom Adapter-module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f62a7685dc9046658544249a753fd7c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LoRA parameter efficient fine-tuning\n",
    "# Parameters are freezed and small submodules with low-rank matrices ar inserted at the target layers.\n",
    "# initialization of model\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\", quantization_config=quantization_config,attn_implementation=\"sdpa\")\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "gemma = get_peft_model(gemma, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,805,824 || all params: 2,515,978,240 || trainable%: 0.3897420034920493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GemmaForCausalLM(\n",
       "      (model): GemmaModel(\n",
       "        (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-17): 18 x GemmaDecoderLayer(\n",
       "            (self_attn): GemmaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): GemmaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): GemmaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=16384, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=16384, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=16384, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): GELUActivation()\n",
       "            )\n",
       "            (input_layernorm): GemmaRMSNorm()\n",
       "            (post_attention_layernorm): GemmaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): GemmaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model-structure and trainable parameters (this can be tuned by hyperparameters)\n",
    "gemma.print_trainable_parameters()\n",
    "gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapter NN for parameter efficient fine-tuning\n",
    "# Adapters (bottleneck feed-forward networks) are added as modules to the layers of the model\n",
    "# adapting attention projections and MLP projections while freezing original model parameters\n",
    "\n",
    "class Adapter(nn.Module):\n",
    "    def __init__(self, size = 6, model_dim = 2048):\n",
    "        super().__init__()\n",
    "        self.adapter_block = nn.Sequential(\n",
    "            nn.Linear(model_dim, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, model_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output = self.adapter_block(x)\n",
    "        adapter_out = output + x\n",
    "\n",
    "        return adapter_out\n",
    "\n",
    "\n",
    "class Adaptered(nn.Module):\n",
    "    def __init__(self, orig_layer):\n",
    "        super().__init__()\n",
    "        self.orig_layer = orig_layer\n",
    "        self.adapter = Adapter()\n",
    "\n",
    "    def forward(self, *x):\n",
    "        orig_out = self.orig_layer(*x)\n",
    "        output = (self.adapter.forward(orig_out[0].unsqueeze(0))[0],)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "class model_with_adapter(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\", quantization_config=self.quantization_config,attn_implementation=\"sdpa\")\n",
    "        # Freeze the original model parameters\n",
    "        for params in self.model.parameters():\n",
    "            params.requires_grad = False\n",
    "        # Embed adapter layers into the transformer blocks \n",
    "        for i, gemma_layer in enumerate(self.model.model.layers):\n",
    "            gemma_layer.self_attn.q_proj = Adaptered(gemma_layer.self_attn.q_proj)\n",
    "            gemma_layer.self_attn.k_proj = Adaptered(gemma_layer.self_attn.k_proj)\n",
    "            gemma_layer.self_attn.v_proj = Adaptered(gemma_layer.self_attn.v_proj)\n",
    "            gemma_layer.self_attn.o_proj = Adaptered(gemma_layer.self_attn.o_proj)\n",
    "    \n",
    "            gemma_layer.mlp.gate_proj = Adaptered(gemma_layer.mlp.gate_proj)\n",
    "            gemma_layer.mlp.up_proj = Adaptered(gemma_layer.mlp.up_proj)\n",
    "            gemma_layer.mlp.down_proj = Adaptered(gemma_layer.mlp.down_proj)\n",
    "\n",
    "    def get_model(self):\n",
    "\n",
    "        return self.model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom get_parameters function\n",
    "def get_parameters(model):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_percentage = (trainable_params / total_params) * 100\n",
    "\n",
    "    trainable_params_str = \"{:,}\".format(trainable_params)\n",
    "    total_params_str = \"{:,}\".format(total_params)\n",
    "\n",
    "    print(f\"trainable params: {trainable_params_str} || all params: {total_params_str} || trainable%: {trainable_percentage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0198f130175d4f38a30e07dbba33415c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,355,380 || all params: 1,518,623,476 || trainable%: 0.2209487771674616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model_with_adapter(\n",
       "  (model): GemmaForCausalLM(\n",
       "    (model): GemmaModel(\n",
       "      (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-17): 18 x GemmaDecoderLayer(\n",
       "          (self_attn): GemmaSdpaAttention(\n",
       "            (q_proj): Adaptered(\n",
       "              (orig_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (adapter): Adapter(\n",
       "                (adapter_block): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=6, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=6, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (k_proj): Adaptered(\n",
       "              (orig_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (adapter): Adapter(\n",
       "                (adapter_block): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=6, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=6, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (v_proj): Adaptered(\n",
       "              (orig_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (adapter): Adapter(\n",
       "                (adapter_block): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=6, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=6, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (o_proj): Adaptered(\n",
       "              (orig_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (adapter): Adapter(\n",
       "                (adapter_block): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=6, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=6, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (rotary_emb): GemmaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): GemmaMLP(\n",
       "            (gate_proj): Adaptered(\n",
       "              (orig_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "              (adapter): Adapter(\n",
       "                (adapter_block): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=6, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=6, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (up_proj): Adaptered(\n",
       "              (orig_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "              (adapter): Adapter(\n",
       "                (adapter_block): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=6, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=6, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (down_proj): Adaptered(\n",
       "              (orig_layer): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
       "              (adapter): Adapter(\n",
       "                (adapter_block): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=6, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=6, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (act_fn): GELUActivation()\n",
       "          )\n",
       "          (input_layernorm): GemmaRMSNorm()\n",
       "          (post_attention_layernorm): GemmaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): GemmaRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialization of adapter-model.\n",
    "# \n",
    "gemma = model_with_adapter().to('cuda')\n",
    "\n",
    "# Model-structure and trainable parameters (this can be tuned by hyperparameters)\n",
    "get_parameters(gemma)\n",
    "gemma\n",
    "\n",
    "# OBS A lot less params, not sure why.. (maybe cause of degeneration? or mistake, need to look into)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projection module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 1024\n",
    "projection_size = 6\n",
    "\n",
    "class ProjectionNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ProjectionNN, self).__init__()\n",
    "\n",
    "        # Architecture\n",
    "        self.fc1 = nn.Linear(embedding_size, 128).cuda()\n",
    "        self.relu = nn.ReLU().cuda()\n",
    "        self.fc2 = nn.Linear(128, 2048 * projection_size).cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(-1,6,2048)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching and preprocessing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_541228/4265014453.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_death_small48['y'] = 1\n",
      "/tmp/ipykernel_541228/4265014453.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_alive_big48['y'] = 0\n",
      "/tmp/ipykernel_541228/4265014453.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_death_big48['y'] = 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(fname)\n",
    "df_death_small48 = df[((df['img_length_of_stay'] < 48) & (df['death_status'] == 1))]\n",
    "df_alive_big48 = df[((df['img_length_of_stay'] >= 48) & (df['death_status'] == 0))]\n",
    "df_death_big48 = df[((df['img_length_of_stay'] >= 48) & (df['death_status'] == 1))]\n",
    "\n",
    "df_death_small48['y'] = 1\n",
    "df_alive_big48['y'] = 0\n",
    "df_death_big48['y'] = 0\n",
    "df = pd.concat([df_death_small48, df_alive_big48, df_death_big48], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     haim_id      vd_0      vd_1      vd_2      vd_3      vd_4      vd_5  \\\n",
      "256     6557  0.005299  0.082119  0.274407  0.017487  0.255308  0.003707   \n",
      "259     6557  0.000000  0.079306  0.381579  0.015250  0.402685  0.011122   \n",
      "267     6558  0.005299  0.082119  0.274407  0.017487  0.255308  0.003707   \n",
      "270     6558  0.000000  0.079306  0.381579  0.015250  0.402685  0.011122   \n",
      "319     6581  0.002288  0.078941  0.088397  0.017775  0.071482  0.006970   \n",
      "\n",
      "         vd_6      vd_7      vd_8  ...   vd_1015   vd_1016   vd_1017  \\\n",
      "256  0.137267  0.024046  0.145395  ...  0.008003  0.013876  0.005360   \n",
      "259  0.125938  0.033254  0.227433  ...  0.042140  0.036560  0.006585   \n",
      "267  0.137267  0.024046  0.145395  ...  0.008003  0.013876  0.005360   \n",
      "270  0.125938  0.033254  0.227433  ...  0.042140  0.036560  0.006585   \n",
      "319  0.223354  0.045017  0.056177  ...  0.004973  0.000343  0.000000   \n",
      "\n",
      "      vd_1018   vd_1019   vd_1020   vd_1021   vd_1022   vd_1023  y  \n",
      "256  0.039292  0.029467  0.003972  0.000203  0.024739  0.005796  1  \n",
      "259  0.042200  0.029051  0.006776  0.000000  0.022821  0.011584  1  \n",
      "267  0.039292  0.029467  0.003972  0.000203  0.024739  0.005796  1  \n",
      "270  0.042200  0.029051  0.006776  0.000000  0.022821  0.011584  1  \n",
      "319  0.005877  0.005696  0.000000  0.000000  0.013369  0.168811  1  \n",
      "\n",
      "[5 rows x 1026 columns]\n"
     ]
    }
   ],
   "source": [
    "vd_cols = df.filter(regex='^vd_')\n",
    "y_col = df[['y']]\n",
    "haim_col = df[['haim_id']]\n",
    "df = pd.concat([haim_col, vd_cols, y_col], axis=1)\n",
    "\n",
    "pkl_list = df['haim_id'].unique().tolist()\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_split(df, pkl_list, test_size=0.3, validation_size=0.1, random_state=None):\n",
    "    # Split into training and test sets\n",
    "    train_set, test_set = train_test_split(pkl_list, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Further split the training set into training and validation sets\n",
    "    train_set, validation_set = train_test_split(train_set, test_size=validation_size, random_state=random_state)\n",
    "\n",
    "    train_idx = df[df['haim_id'].isin(train_set)]['haim_id'].tolist()\n",
    "    validation_idx = df[df['haim_id'].isin(validation_set)]['haim_id'].tolist()\n",
    "    test_idx = df[df['haim_id'].isin(test_set)]['haim_id'].tolist()\n",
    "\n",
    "    x_train = df[df['haim_id'].isin(train_idx)].drop(['haim_id', 'y'], axis=1).values\n",
    "    x_validation = df[df['haim_id'].isin(validation_idx)].drop(['haim_id', 'y'], axis=1).values\n",
    "    x_test = df[df['haim_id'].isin(test_idx)].drop(['haim_id', 'y'], axis=1).values\n",
    "\n",
    "    y_train = df[df['haim_id'].isin(train_idx)]['y'].values\n",
    "    y_validation = df[df['haim_id'].isin(validation_idx)]['y'].values\n",
    "    y_test = df[df['haim_id'].isin(test_idx)]['y'].values\n",
    "\n",
    "    return x_train, x_validation, x_test, y_train, y_validation, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, x_test, y_train, y_val, y_test, = data_split(df, pkl_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt function to be fed into training loop\n",
    "\n",
    "def formatting_func(example, emb, label):\n",
    "    text = f\"### INSTRUCTION: {'Use this input to create the correct label.'}\\n### INPUT: {emb}\\n### LABEL: {label}\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, embedding, labels):\n",
    "        self.labels = labels\n",
    "        self.embedding = embedding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        emb = self.embedding[idx]\n",
    "        sample = {\"Emb\": emb, \"Class\": label}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_batch for strings\n",
    "\n",
    "def collate_batch(batch):\n",
    "     \n",
    "    emb_list, classes = [], []\n",
    "    for thing in batch:\n",
    "        #print(batch)\n",
    "        emb_list.append(thing['Emb'])\n",
    "        classes.append(tokenizer(thing['Class'], return_tensors=\"pt\"))\n",
    "    text = torch.tensor(emb_list)\n",
    "    classes = torch.cat([item['input_ids'] for item in classes], dim=0)\n",
    "    return text, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_batch for 0/1 labels\n",
    "\n",
    "def collate_batch(batch):\n",
    "     \n",
    "    emb_list, classes = [], []\n",
    "    for thing in batch:\n",
    "        #print(batch)\n",
    "        emb_list.append(thing['Emb'])\n",
    "        classes.append(thing['Class'])\n",
    "    text = torch.tensor(emb_list)\n",
    "    classes = torch.tensor(classes, dtype=torch.float16)\n",
    "    return text, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProjectionNN()\n",
    "\n",
    "Trainset = CustomDataset(x_train.tolist(), y_train)\n",
    "Valset = CustomDataset(x_val, y_val)\n",
    "Testset = CustomDataset(x_test, y_test)\n",
    "\n",
    "Train_loader = DataLoader(Trainset, batch_size=1, collate_fn=collate_batch)\n",
    "Val_loader = DataLoader(Valset, batch_size=1, collate_fn=collate_batch)\n",
    "Test_loader = DataLoader(Testset, batch_size=1, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_output(emb, gemma):\n",
    "    outputs = gemma(inputs_embeds=emb)\n",
    "    noyes = [1294,3553]\n",
    "    logits = outputs['logits']\n",
    "    logits = logits[:,1,noyes]\n",
    "    return logits\n",
    "\n",
    "def output_to_label(logits):\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_token_id = torch.argmax(probs, dim=-1)\n",
    "    return predicted_token_id\n",
    "\n",
    "    \n",
    "def train_epoch(model, gemma, optimizer, loss_fn, train_loader, device):\n",
    "    # Train:\n",
    "    model.train()\n",
    "    train_loss_batches, train_acc_batches = [], []\n",
    "    num_batches = len(train_loader)\n",
    "    for batch_index, (x, y) in enumerate(train_loader, 1):\n",
    "        inputs, labels = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        emb = model(inputs)\n",
    "        logits = custom_output(emb.to(torch.float16), gemma)\n",
    "        \n",
    "        loss = loss_fn(logits, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_batches.append(loss.item())\n",
    "\n",
    "        hard_preds = output_to_label(logits)\n",
    "        acc_batch_avg = (hard_preds == labels).float().mean().item()\n",
    "        train_acc_batches.append(acc_batch_avg)\n",
    "\n",
    "    return model, train_loss_batches, train_acc_batches\n",
    "\n",
    "def validate(model, gemma, loss_fn, val_loader, device, word_embs):\n",
    "    val_loss_cum = 0\n",
    "    val_acc_cum = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (x, y) in enumerate(val_loader, 1):\n",
    "            inputs, labels = x.to(device), y.to(device)\n",
    "            emb = model.forward(inputs)\n",
    "            concatted = torch.cat((word_embs,emb), dim=1).to(torch.float16)\n",
    "            logits = custom_output(concatted, gemma)\n",
    "\n",
    "            batch_loss = loss_fn(logits, labels.float())\n",
    "            val_loss_cum += batch_loss.item()\n",
    "            hard_preds = output_to_label(logits)\n",
    "            acc_batch_avg = (hard_preds == labels).float().mean().item()\n",
    "            val_acc_cum += acc_batch_avg\n",
    "    return val_loss_cum/len(val_loader), val_acc_cum/len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, gemma, optimizer, loss_fn, train_loader, val_loader, num_epochs):\n",
    "    print(\"Starting training\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model, train_loss, train_acc = train_epoch(model, gemma,\n",
    "                                                   optimizer,\n",
    "                                                   loss_fn,\n",
    "                                                   train_loader,\n",
    "                                                   device)\n",
    "        val_loss, val_acc = validate(model, loss_fn, val_loader, device)\n",
    "        print(f\"Epoch {epoch}/{num_epochs}: \"\n",
    "              f\"Train loss: {sum(train_loss)/len(train_loss):.3f}, \"\n",
    "              f\"Train acc.: {sum(train_acc)/len(train_acc):.3f}, \"\n",
    "              f\"Val. loss: {val_loss:.3f}, \"\n",
    "              f\"Val. acc.: {val_acc:.3f}\")\n",
    "        train_losses.extend(train_loss)\n",
    "        train_accs.extend(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "    return model, train_losses, train_accs, val_losses, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edgelab/miniconda3/envs/gemma/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([1])) must be the same as input size (torch.Size([1, 2]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m      2\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgemma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVal_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(model, gemma, optimizer, loss_fn, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[1;32m      5\u001b[0m train_losses, train_accs, val_losses, val_accs \u001b[38;5;241m=\u001b[39m [], [], [], []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m     model, train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgemma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m validate(model, loss_fn, val_loader, device)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(train_loss)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loss)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain acc.: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(train_acc)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_acc)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal. loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal. acc.: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 26\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, gemma, optimizer, loss_fn, train_loader, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m emb \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     24\u001b[0m logits \u001b[38;5;241m=\u001b[39m custom_output(emb\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat16), gemma)\n\u001b[0;32m---> 26\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/home/edgelab/miniconda3/envs/gemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/home/edgelab/miniconda3/envs/gemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/edgelab/miniconda3/envs/gemma/lib/python3.12/site-packages/torch/nn/modules/loss.py:725\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 725\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39;49m, target,\n\u001b[1;32m    726\u001b[0m                                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    727\u001b[0m                                               pos_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_weight,\n\u001b[1;32m    728\u001b[0m                                               reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m/home/edgelab/miniconda3/envs/gemma/lib/python3.12/site-packages/torch/nn/functional.py:3197\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3194\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3196\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (target\u001b[39m.\u001b[39msize() \u001b[39m==\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()):\n\u001b[0;32m-> 3197\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTarget size (\u001b[39m\u001b[39m{\u001b[39;00mtarget\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m) must be the same as input size (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   3199\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([1])) must be the same as input size (torch.Size([1, 2]))"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "training_loop(model, gemma, optimizer, loss_fn, Train_loader, Val_loader, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out finetune of generative aspect of gemma\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "embedding_matrix = gemma.get_input_embeddings().weight\n",
    "prompt = \"### PROMPT: Did this patient stay longer than 48 h?\"\n",
    "prompt_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "#print(prompt_ids[\"input_ids\"].shape)\n",
    "prompt_embeddings = [embedding_matrix[token_id].to('cuda') for token_id in prompt_ids[\"input_ids\"][0]]\n",
    "#print(lab.shape)\n",
    "prompt_embeddings = torch.stack(prompt_embeddings)\n",
    "label_embeddings = [embedding_matrix[token_id].to('cuda') for token_id in lab.to(dtype=torch.long)]\n",
    "label_embeddings = torch.stack(label_embeddings)\n",
    "\n",
    "p_mod = model(mod)\n",
    "\n",
    "conc_emb = torch.cat([prompt_embeddings.unsqueeze(0).to(dtype=torch.float16), p_mod.to('cuda'), label_embeddings.to(dtype=torch.float16)], dim=1)\n",
    "\n",
    "\n",
    "\n",
    "conc_emb.to(dtype=torch.float16).to('cuda')\n",
    "output = gemma(inputs_embeds=conc_emb)\n",
    "\n",
    "\n",
    "probabilities = F.softmax(output['logits'], dim=-1)\n",
    "predicted_ids = torch.argmax(probabilities, dim=-1)\n",
    "\n",
    "tokenizer.decode(predicted_ids[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.2 ('gemma')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10601c873fb2576e1e1a48994b394f584387b3d54a28d8ac07023c991446672f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
