{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for testing mutlimodal capability of Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Filepath to embeddings\n",
    "fname = \"/mnt/mimic/data/HAIM/mimic_extras/embeddings.csv\"\n",
    "\n",
    "# YES-TOKEN: 3276\n",
    "# NO-TOKEN: 956"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03396531fc4f455d8cbb196c219da8bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, \n",
    "                                         bnb_4bit_use_double_quant=True,\n",
    "                                         bnb_4bit_quant_type=\"nf4\",\n",
    "                                         bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\", quantization_config=quantization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 1024\n",
    "projection_size = 6\n",
    "\n",
    "class ProjectionNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ProjectionNN, self).__init__()\n",
    "\n",
    "        # Architecture\n",
    "        self.fc1 = nn.Linear(embedding_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 2048 * projection_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(-1,6,2048)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(fname)\n",
    "\n",
    "condition_death_small48 = (df['img_length_of_stay'] < 48) & (df['death_status'] == 1)\n",
    "condition_alive_big48 = (df['img_length_of_stay'] >= 48) & (df['death_status'] == 0)\n",
    "condition_death_big48 = (df['img_length_of_stay'] >= 48) & (df['death_status'] == 1)\n",
    "\n",
    "y = [0]*len(df)\n",
    "for i, condition in enumerate(condition_death_small48):\n",
    "    if condition:\n",
    "        y[i] = 1\n",
    "\n",
    "# Use .loc to avoid SettingWithCopyWarning\n",
    "#df.loc[condition_death_small48, 'y'] = 1\n",
    "#df.loc[condition_alive_big48, 'y'] = 0\n",
    "#df.loc[condition_death_big48, 'y'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   haim_id      vd_0      vd_1      vd_2      vd_3      vd_4      vd_5  \\\n",
      "0     6514  0.000000  0.102385  0.188977  0.007367  0.219433  0.000106   \n",
      "1     6514  0.000399  0.063669  0.297278  0.007873  0.288133  0.000000   \n",
      "2     6515  0.000000  0.073280  0.390735  0.007879  0.094356  0.006252   \n",
      "3     6515  0.000000  0.003337  0.084882  0.008524  0.030514  0.000936   \n",
      "4     6515  0.000121  0.098648  0.514754  0.001866  0.211975  0.011927   \n",
      "\n",
      "       vd_6      vd_7      vd_8  ...   vd_1015   vd_1016   vd_1017   vd_1018  \\\n",
      "0  0.074859  0.017974  0.138016  ...  0.010239  0.000589  0.000743  0.102930   \n",
      "1  0.099269  0.004799  0.215243  ...  0.000000  0.013072  0.000000  0.078393   \n",
      "2  0.113489  0.021230  0.324026  ...  0.173980  0.009676  0.095614  0.052150   \n",
      "3  0.242137  0.027981  0.025548  ...  0.071969  0.000301  0.142212  0.017643   \n",
      "4  0.081207  0.010555  0.364878  ...  0.204686  0.013269  0.134133  0.044195   \n",
      "\n",
      "    vd_1019   vd_1020   vd_1021   vd_1022   vd_1023  y  \n",
      "0  0.008906  0.000000  0.000000  0.003800  0.170845  0  \n",
      "1  0.027688  0.000000  0.002452  0.018334  0.133168  0  \n",
      "2  0.026880  0.047541  0.001939  0.017099  0.011062  0  \n",
      "3  0.000300  0.026498  0.111101  0.001279  0.017622  0  \n",
      "4  0.023045  0.062860  0.000953  0.007688  0.037414  0  \n",
      "\n",
      "[5 rows x 1026 columns]\n"
     ]
    }
   ],
   "source": [
    "vd_cols = df.filter(regex='^vd_')\n",
    "y_col = pd.Series(y, name='y')\n",
    "haim_col = df[['haim_id']]\n",
    "df = pd.concat([haim_col, vd_cols, y_col], axis=1)\n",
    "\n",
    "pkl_list = df['haim_id'].unique().tolist()\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup functions for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dataset class*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, vectors, labels):\n",
    "        self.vectors = vectors\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vectors)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        vector = torch.tensor(self.vectors[index]).float()\n",
    "        label = torch.from_numpy(np.array([self.labels[index]]))\n",
    "        return vector, label.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Data splitter*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(df, pkl_list):\n",
    "    train_id, test_id = train_test_split(pkl_list, test_size=0.3)\n",
    "    \n",
    "    train_idx = df[df['haim_id'].isin(train_id)]['haim_id'].tolist()\n",
    "    test_idx = df[df['haim_id'].isin(test_id)]['haim_id'].tolist()\n",
    "\n",
    "    x_train = df[df['haim_id'].isin(train_idx)].drop(['haim_id','y'],axis=1).values\n",
    "    x_test = df[df['haim_id'].isin(test_idx)].drop(['haim_id','y'],axis=1).values\n",
    "\n",
    "    y_train = df[df['haim_id'].isin(train_idx)]['y'].values\n",
    "    y_test = df[df['haim_id'].isin(test_idx)]['y'].values\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Train/Val funcs (needs to be updated)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_output(emb, gemma):\n",
    "    outputs = gemma(inputs_embeds=emb)\n",
    "    noyes = [956, 3276]\n",
    "    logits = outputs['logits']\n",
    "    logits = logits[:,1,noyes]\n",
    "    return logits\n",
    "\n",
    "def output_to_label(logits):\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_token_id = torch.argmax(probs, dim=-1)\n",
    "    return predicted_token_id\n",
    "\n",
    "    \n",
    "def train_epoch(model, gemma, optimizer, loss_fn, train_loader, device, word_embs):\n",
    "    # Train:\n",
    "    model.train()\n",
    "    train_loss_batches, train_acc_batches = [], []\n",
    "    for batch_index, (x, y) in enumerate(train_loader, 1):\n",
    "        inputs, labels = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        emb = model.forward(inputs)\n",
    "        word_embs_extended = word_embs.repeat(len(inputs),1,1).detach()\n",
    "\n",
    "        concatted = torch.cat((word_embs_extended, emb), dim=1).to(torch.float16)\n",
    "        logits = custom_output(concatted, gemma).float()\n",
    "        \n",
    "        loss = loss_fn(logits, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_batches.append(loss.item())\n",
    "\n",
    "        hard_preds = output_to_label(logits)\n",
    "        acc_batch_avg = (hard_preds == labels).float().mean().item()\n",
    "        train_acc_batches.append(acc_batch_avg)\n",
    "\n",
    "    return model, train_loss_batches, train_acc_batches\n",
    "\n",
    "def validate(model, gemma, loss_fn, val_loader, device, word_embs):\n",
    "    val_loss_cum = 0\n",
    "    val_acc_cum = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (x, y) in enumerate(val_loader, 1):\n",
    "            inputs, labels = x.to(device), y.to(device)\n",
    "\n",
    "            emb = model.forward(inputs)\n",
    "            word_embs_extended = word_embs.repeat(len(inputs),1,1).detach()\n",
    "\n",
    "            concatted = torch.cat((word_embs_extended, emb), dim=1).to(torch.float16)\n",
    "            logits = custom_output(concatted, gemma)\n",
    "\n",
    "            batch_loss = loss_fn(logits, labels.long())\n",
    "            val_loss_cum += batch_loss.item()\n",
    "            hard_preds = output_to_label(logits)\n",
    "            acc_batch_avg = (hard_preds == labels).float().mean().item()\n",
    "            val_acc_cum += acc_batch_avg\n",
    "    return val_loss_cum/len(val_loader), val_acc_cum/len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Training framework*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, gemma, optimizer, loss_fn, train_loader, val_loader, num_epochs, word_embs):\n",
    "    print(\"Starting training\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model, train_loss, train_acc = train_epoch(model,\n",
    "                                                   gemma,\n",
    "                                                   optimizer,\n",
    "                                                   loss_fn,\n",
    "                                                   train_loader,\n",
    "                                                   device,\n",
    "                                                   word_embs)\n",
    "        val_loss, val_acc = validate(model, gemma, loss_fn, val_loader, device, word_embs)\n",
    "        print(f\"Epoch {epoch}/{num_epochs}: \"\n",
    "              f\"Train loss: {sum(train_loss)/len(train_loss):.3f}, \"\n",
    "              f\"Train acc.: {sum(train_acc)/len(train_acc):.3f}, \"\n",
    "              f\"Val. loss: {val_loss:.3f}, \"\n",
    "              f\"Val. acc.: {val_acc:.3f}\")\n",
    "        train_losses.extend(train_loss)\n",
    "        train_accs.extend(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "    return model, train_losses, train_accs, val_losses, val_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Main*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Tell me a story\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "word_embs = gemma.get_input_embeddings().weight[input_ids.input_ids].to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch 1/5: Train loss: 0.494, Train acc.: 0.990, Val. loss: 0.386, Val. acc.: 0.984\n",
      "Epoch 2/5: Train loss: 0.494, Train acc.: 0.990, Val. loss: 0.386, Val. acc.: 0.984\n",
      "Epoch 3/5: Train loss: 0.376, Train acc.: 0.990, Val. loss: 0.386, Val. acc.: 0.984\n",
      "Epoch 4/5: Train loss: 0.494, Train acc.: 0.990, Val. loss: 0.386, Val. acc.: 0.984\n",
      "Epoch 5/5: Train loss: 0.494, Train acc.: 0.990, Val. loss: 0.386, Val. acc.: 0.984\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "x_train, x_val, y_train, y_val = data_split(df, pkl_list)\n",
    "x_train_small, x_val_small, y_train_small, y_val_small = data_split(df.iloc[:500], pkl_list)\n",
    "train_set = CustomDataset(x_train_small, y_train_small)\n",
    "val_set = CustomDataset(x_val_small, y_val_small)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=5)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, num_workers=5)\n",
    "\n",
    "w0 = 1 - sum(y_train_small == 0)/len(y_train_small)\n",
    "w1 = 1 - sum(y_train_small == 1)/len(y_train_small)\n",
    "weights = torch.tensor([w0, w1], dtype = torch.float).to(\"cuda\")\n",
    "\n",
    "model = ProjectionNN()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "fine_tuned, train_losses, train_accs, val_losses, val_accs = training_loop(model, gemma, optimizer, loss_fn, train_loader, val_loader, num_epochs, word_embs)\n",
    "\n",
    "torch.save(fine_tuned, 'finetuned.pth')\n",
    "\n",
    "with open('train_losses.pkl', 'wb') as f1:\n",
    "    pickle.dump(train_losses, f1)\n",
    "\n",
    "with open('train_accs.pkl', 'wb') as f2:\n",
    "    pickle.dump(train_accs, f2)\n",
    "\n",
    "with open('val_losses.pkl', 'wb') as f3:\n",
    "    pickle.dump(val_losses, f3)\n",
    "\n",
    "with open('val_accs.pkl', 'wb') as f4:\n",
    "    pickle.dump(val_accs, f4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out gemma instruct on text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, _, _, _ = data_split(df, pkl_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProjectionNN(\n",
       "  (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=12288, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('finetuned_small.pth')\n",
    "model.eval().to(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Obrador Obrador's Obrador's pronouncements.\":\n",
      "\n",
      "**Story:**\n",
      "\n",
      "One sunny afternoon, Obrador announced a sm√∂r session for the Obrador's Obrador's supporters, but the session was cancelled at\n"
     ]
    }
   ],
   "source": [
    "emb = torch.tensor(train[0]).float().to(device='cuda')\n",
    "\n",
    "projected = model(emb).to(device='cuda').to(torch.float16)\n",
    "\n",
    "concatted = torch.cat((word_embs, projected), dim=1).to(torch.float16)\n",
    "\n",
    "outputs = gemma.generate(inputs_embeds = concatted, max_new_tokens = 40)\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
