{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning of gemma-2b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL THE NECESSARY IMPORTS\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, HfArgumentParser, TrainingArguments\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "from peft import LoraConfig, TaskType, get_peft_model, get_peft_config\n",
    "\n",
    "# Filepath to embeddings\n",
    "fname = \"/mnt/mimic/data/HAIM/mimic_extras/embeddings.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the model\n",
    "\n",
    "Different versions, with huggingface LoRA-class or custom Adapter-module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc5e45d861543b488ad6a38426a89fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LoRA parameter efficient fine-tuning\n",
    "# Parameters are freezed and small submodules with low-rank matrices ar inserted at the target layers.\n",
    "# initialization of model\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\", quantization_config=quantization_config,attn_implementation=\"sdpa\")\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,805,824 || all params: 2,515,978,240 || trainable%: 0.3897420034920493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GemmaForCausalLM(\n",
       "      (model): GemmaModel(\n",
       "        (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-17): 18 x GemmaDecoderLayer(\n",
       "            (self_attn): GemmaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): GemmaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): GemmaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=16384, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=16384, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=16384, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): GELUActivation()\n",
       "            )\n",
       "            (input_layernorm): GemmaRMSNorm()\n",
       "            (post_attention_layernorm): GemmaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): GemmaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model-structure and trainable parameters (this can be tuned by hyperparameters)\n",
    "model.print_trainable_parameters()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapter NN for parameter efficient fine-tuning\n",
    "# Adapters (bottleneck feed-forward networks) are added as modules to the layers of the model\n",
    "# adapting attention projections and MLP projections while freezing original model parameters\n",
    "\n",
    "class Adapter(nn.Module):\n",
    "    def __init__(self, size = 6, model_dim = 2048):\n",
    "        super().__init__()\n",
    "        self.adapter_block = nn.Sequential(\n",
    "            nn.Linear(model_dim, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, model_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output = self.adapter_block(x)\n",
    "        adapter_out = output + x\n",
    "\n",
    "        return adapter_out\n",
    "\n",
    "\n",
    "class Adaptered(nn.Module):\n",
    "    def __init__(self, orig_layer):\n",
    "        super().__init__()\n",
    "        self.orig_layer = orig_layer\n",
    "        self.adapter = Adapter()\n",
    "\n",
    "    def forward(self, *x):\n",
    "        orig_out = self.orig_layer(*x)\n",
    "        output = (self.adapter.forward(orig_out[0].unsqueeze(0))[0],)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "class model_with_adapter(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\", quantization_config=self.quantization_config,attn_implementation=\"sdpa\")\n",
    "        # Freeze the original model parameters\n",
    "        for params in self.model.parameters():\n",
    "            params.requires_grad = False\n",
    "        # Embed adapter layers into the transformer blocks \n",
    "        for i, gemma_layer in enumerate(self.model.model.layers):\n",
    "            gemma_layer.self_attn.q_proj = Adaptered(gemma_layer.self_attn.q_proj)\n",
    "            gemma_layer.self_attn.k_proj = Adaptered(gemma_layer.self_attn.k_proj)\n",
    "            gemma_layer.self_attn.v_proj = Adaptered(gemma_layer.self_attn.v_proj)\n",
    "            gemma_layer.self_attn.o_proj = Adaptered(gemma_layer.self_attn.o_proj)\n",
    "    \n",
    "            gemma_layer.mlp.gate_proj = Adaptered(gemma_layer.mlp.gate_proj)\n",
    "            gemma_layer.mlp.up_proj = Adaptered(gemma_layer.mlp.up_proj)\n",
    "            gemma_layer.mlp.down_proj = Adaptered(gemma_layer.mlp.down_proj)\n",
    "\n",
    "    def get_model(self):\n",
    "\n",
    "        return self.model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom get_parameters function\n",
    "def get_parameters(model):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_percentage = (trainable_params / total_params) * 100\n",
    "\n",
    "    trainable_params_str = \"{:,}\".format(trainable_params)\n",
    "    total_params_str = \"{:,}\".format(total_params)\n",
    "\n",
    "    print(f\"trainable params: {trainable_params_str} || all params: {total_params_str} || trainable%: {trainable_percentage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3838b207048e49a199c7aa1d6690d30e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,355,380 || all params: 1,518,623,476 || trainable%: 0.2209487771674616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model_with_adapter(\n",
       "  (model): GemmaForCausalLM(\n",
       "    (model): GemmaModel(\n",
       "      (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-17): 18 x GemmaDecoderLayer(\n",
       "          (self_attn): GemmaSdpaAttention(\n",
       "            (q_proj): Adaptered(\n",
       "              (orig_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (adapter): Adapter(\n",
       "                (adapter_block): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=6, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=6, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (k_proj): Adaptered(\n",
       "              (orig_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (adapter): Adapter(\n",
       "                (adapter_block): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=6, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=6, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (v_proj): Adaptered(\n",
       "              (orig_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (adapter): Adapter(\n",
       "                (adapter_block): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=6, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=6, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (o_proj): Adaptered(\n",
       "              (orig_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (adapter): Adapter(\n",
       "                (adapter_block): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=6, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=6, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (rotary_emb): GemmaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): GemmaMLP(\n",
       "            (gate_proj): Adaptered(\n",
       "              (orig_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "              (adapter): Adapter(\n",
       "                (adapter_block): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=6, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=6, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (up_proj): Adaptered(\n",
       "              (orig_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "              (adapter): Adapter(\n",
       "                (adapter_block): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=6, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=6, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (down_proj): Adaptered(\n",
       "              (orig_layer): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
       "              (adapter): Adapter(\n",
       "                (adapter_block): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=6, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=6, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (act_fn): GELUActivation()\n",
       "          )\n",
       "          (input_layernorm): GemmaRMSNorm()\n",
       "          (post_attention_layernorm): GemmaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): GemmaRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialization of adapter-model.\n",
    "# \n",
    "model = model_with_adapter().to('cuda')\n",
    "\n",
    "# Model-structure and trainable parameters (this can be tuned by hyperparameters)\n",
    "get_parameters(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching and preprocessing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30739/4265014453.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_death_small48['y'] = 1\n",
      "/tmp/ipykernel_30739/4265014453.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_alive_big48['y'] = 0\n",
      "/tmp/ipykernel_30739/4265014453.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_death_big48['y'] = 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(fname)\n",
    "df_death_small48 = df[((df['img_length_of_stay'] < 48) & (df['death_status'] == 1))]\n",
    "df_alive_big48 = df[((df['img_length_of_stay'] >= 48) & (df['death_status'] == 0))]\n",
    "df_death_big48 = df[((df['img_length_of_stay'] >= 48) & (df['death_status'] == 1))]\n",
    "\n",
    "df_death_small48['y'] = 1\n",
    "df_alive_big48['y'] = 0\n",
    "df_death_big48['y'] = 0\n",
    "df = pd.concat([df_death_small48, df_alive_big48, df_death_big48], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     haim_id      vd_0      vd_1      vd_2      vd_3      vd_4      vd_5  \\\n",
      "256     6557  0.005299  0.082119  0.274407  0.017487  0.255308  0.003707   \n",
      "259     6557  0.000000  0.079306  0.381579  0.015250  0.402685  0.011122   \n",
      "267     6558  0.005299  0.082119  0.274407  0.017487  0.255308  0.003707   \n",
      "270     6558  0.000000  0.079306  0.381579  0.015250  0.402685  0.011122   \n",
      "319     6581  0.002288  0.078941  0.088397  0.017775  0.071482  0.006970   \n",
      "\n",
      "         vd_6      vd_7      vd_8  ...   vd_1015   vd_1016   vd_1017  \\\n",
      "256  0.137267  0.024046  0.145395  ...  0.008003  0.013876  0.005360   \n",
      "259  0.125938  0.033254  0.227433  ...  0.042140  0.036560  0.006585   \n",
      "267  0.137267  0.024046  0.145395  ...  0.008003  0.013876  0.005360   \n",
      "270  0.125938  0.033254  0.227433  ...  0.042140  0.036560  0.006585   \n",
      "319  0.223354  0.045017  0.056177  ...  0.004973  0.000343  0.000000   \n",
      "\n",
      "      vd_1018   vd_1019   vd_1020   vd_1021   vd_1022   vd_1023  y  \n",
      "256  0.039292  0.029467  0.003972  0.000203  0.024739  0.005796  1  \n",
      "259  0.042200  0.029051  0.006776  0.000000  0.022821  0.011584  1  \n",
      "267  0.039292  0.029467  0.003972  0.000203  0.024739  0.005796  1  \n",
      "270  0.042200  0.029051  0.006776  0.000000  0.022821  0.011584  1  \n",
      "319  0.005877  0.005696  0.000000  0.000000  0.013369  0.168811  1  \n",
      "\n",
      "[5 rows x 1026 columns]\n"
     ]
    }
   ],
   "source": [
    "vd_cols = df.filter(regex='^vd_')\n",
    "y_col = df[['y']]\n",
    "haim_col = df[['haim_id']]\n",
    "df = pd.concat([haim_col, vd_cols, y_col], axis=1)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt function to be fed into training loop\n",
    "\n",
    "def formatting_func(example):\n",
    "    text = f\"### INSTRUCTION: {'Use this input to create the correct label.'}\\n### INPUT: {example['modality_embedding']}\\n### LABEL: {example['label']}\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.2 ('gemma')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10601c873fb2576e1e1a48994b394f584387b3d54a28d8ac07023c991446672f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
