{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for testing mutlimodal capability of Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Filepath to embeddings\n",
    "fname = \"/mnt/mimic/data/HAIM/mimic_extras/embeddings.csv\"\n",
    "\n",
    "# YES-TOKEN: 3553\n",
    "# NO-TOKEN: 1294"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "222cda4cb8234d018e517c34bb7c84e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\", quantization_config=quantization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 1024\n",
    "projection_size = 6\n",
    "\n",
    "class ProjectionNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ProjectionNN, self).__init__()\n",
    "\n",
    "        # Architecture\n",
    "        self.fc1 = nn.Linear(embedding_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 2048 * projection_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(-1,6,2048)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(fname)\n",
    "condition_death_small48 = (df['img_length_of_stay'] < 48) & (df['death_status'] == 1)\n",
    "condition_alive_big48 = (df['img_length_of_stay'] >= 48) & (df['death_status'] == 0)\n",
    "condition_death_big48 = (df['img_length_of_stay'] >= 48) & (df['death_status'] == 1)\n",
    "\n",
    "# Use .loc to avoid SettingWithCopyWarning\n",
    "df.loc[condition_death_small48, 'y'] = 1\n",
    "df.loc[condition_alive_big48, 'y'] = 0\n",
    "df.loc[condition_death_big48, 'y'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   haim_id      vd_0      vd_1      vd_2      vd_3      vd_4      vd_5  \\\n",
      "0     6514  0.000000  0.102385  0.188977  0.007367  0.219433  0.000106   \n",
      "1     6514  0.000399  0.063669  0.297278  0.007873  0.288133  0.000000   \n",
      "2     6515  0.000000  0.073280  0.390735  0.007879  0.094356  0.006252   \n",
      "3     6515  0.000000  0.003337  0.084882  0.008524  0.030514  0.000936   \n",
      "4     6515  0.000121  0.098648  0.514754  0.001866  0.211975  0.011927   \n",
      "\n",
      "       vd_6      vd_7      vd_8  ...   vd_1015   vd_1016   vd_1017   vd_1018  \\\n",
      "0  0.074859  0.017974  0.138016  ...  0.010239  0.000589  0.000743  0.102930   \n",
      "1  0.099269  0.004799  0.215243  ...  0.000000  0.013072  0.000000  0.078393   \n",
      "2  0.113489  0.021230  0.324026  ...  0.173980  0.009676  0.095614  0.052150   \n",
      "3  0.242137  0.027981  0.025548  ...  0.071969  0.000301  0.142212  0.017643   \n",
      "4  0.081207  0.010555  0.364878  ...  0.204686  0.013269  0.134133  0.044195   \n",
      "\n",
      "    vd_1019   vd_1020   vd_1021   vd_1022   vd_1023    y  \n",
      "0  0.008906  0.000000  0.000000  0.003800  0.170845  0.0  \n",
      "1  0.027688  0.000000  0.002452  0.018334  0.133168  0.0  \n",
      "2  0.026880  0.047541  0.001939  0.017099  0.011062  0.0  \n",
      "3  0.000300  0.026498  0.111101  0.001279  0.017622  0.0  \n",
      "4  0.023045  0.062860  0.000953  0.007688  0.037414  0.0  \n",
      "\n",
      "[5 rows x 1026 columns]\n"
     ]
    }
   ],
   "source": [
    "vd_cols = df.filter(regex='^vd_')\n",
    "y_col = df[['y']]\n",
    "haim_col = df[['haim_id']]\n",
    "df = pd.concat([haim_col, vd_cols, y_col], axis=1)\n",
    "\n",
    "pkl_list = df['haim_id'].unique().tolist()\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup functions for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Data splitter*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(df, pkl_list):\n",
    "    train_id, test_id = train_test_split(pkl_list, test_size=0.3)\n",
    "    \n",
    "    train_idx = df[df['haim_id'].isin(train_id)]['haim_id'].tolist()\n",
    "    test_idx = df[df['haim_id'].isin(test_id)]['haim_id'].tolist()\n",
    "\n",
    "    x_train = df[df['haim_id'].isin(train_idx)].drop(['haim_id','y'],axis=1).values\n",
    "    x_test = df[df['haim_id'].isin(test_idx)].drop(['haim_id','y'],axis=1).values\n",
    "\n",
    "    y_train = df[df['haim_id'].isin(train_idx)]['y'].values\n",
    "    y_test = df[df['haim_id'].isin(test_idx)]['y'].values\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Train/Val funcs (needs to be updated)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_output(emb, gemma):\n",
    "    outputs = gemma(inputs_embeds=emb)\n",
    "    noyes = [1294,3553]\n",
    "    logits = outputs['logits']\n",
    "    logits = logits[:,1,noyes]\n",
    "    return logits\n",
    "\n",
    "def output_to_label(logits):\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_token_id = torch.argmax(probs, dim=-1)\n",
    "    return predicted_token_id\n",
    "\n",
    "    \n",
    "def train_epoch(model, gemma, optimizer, loss_fn, train_loader, device, word_embs):\n",
    "    # Train:\n",
    "    model.train()\n",
    "    train_loss_batches, train_acc_batches = [], []\n",
    "    num_batches = len(train_loader)\n",
    "    for batch_index, (x, y) in enumerate(train_loader, 1):\n",
    "        inputs, labels = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        emb = model.forward(inputs)\n",
    "        concatted = torch.cat((word_embs,emb), dim=1).to(torch.float16)\n",
    "        logits = custom_output(concatted, gemma)\n",
    "        \n",
    "        loss = loss_fn(logits, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_batches.append(loss.item())\n",
    "\n",
    "        hard_preds = output_to_label(logits)\n",
    "        acc_batch_avg = (hard_preds == labels).float().mean().item()\n",
    "        train_acc_batches.append(acc_batch_avg)\n",
    "\n",
    "    return model, train_loss_batches, train_acc_batches\n",
    "\n",
    "def validate(model, gemma, loss_fn, val_loader, device, word_embs):\n",
    "    val_loss_cum = 0\n",
    "    val_acc_cum = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (x, y) in enumerate(val_loader, 1):\n",
    "            inputs, labels = x.to(device), y.to(device)\n",
    "            emb = model.forward(inputs)\n",
    "            concatted = torch.cat((word_embs,emb), dim=1).to(torch.float16)\n",
    "            logits = custom_output(concatted, gemma)\n",
    "\n",
    "            batch_loss = loss_fn(logits, labels.float())\n",
    "            val_loss_cum += batch_loss.item()\n",
    "            hard_preds = output_to_label(logits)\n",
    "            acc_batch_avg = (hard_preds == labels).float().mean().item()\n",
    "            val_acc_cum += acc_batch_avg\n",
    "    return val_loss_cum/len(val_loader), val_acc_cum/len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Training framework*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, gemma, optimizer, loss_fn, train_loader, val_loader, num_epochs, word_embs):\n",
    "    print(\"Starting training\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model, train_loss, train_acc = train_epoch(model,\n",
    "                                                   gemma,\n",
    "                                                   optimizer,\n",
    "                                                   loss_fn,\n",
    "                                                   train_loader,\n",
    "                                                   val_loader,\n",
    "                                                   device,\n",
    "                                                   word_embs)\n",
    "        val_loss, val_acc = validate(model, loss_fn, val_loader, device)\n",
    "        print(f\"Epoch {epoch}/{num_epochs}: \"\n",
    "              f\"Train loss: {sum(train_loss)/len(train_loss):.3f}, \"\n",
    "              f\"Train acc.: {sum(train_acc)/len(train_acc):.3f}, \"\n",
    "              f\"Val. loss: {val_loss:.3f}, \"\n",
    "              f\"Val. acc.: {val_acc:.3f}\")\n",
    "        train_losses.extend(train_loss)\n",
    "        train_accs.extend(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "    return model, train_losses, train_accs, val_losses, val_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out gemma instruct on text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2, 4521]], device='cuda:0')\n",
      "tensor([[[   3.1172,    7.4336,   44.9062,  ...,   -5.1055,   -1.9609,\n",
      "             3.5332],\n",
      "         [ -30.8125,  -10.2188,  -24.0625,  ...,  -26.6250,  -28.3281,\n",
      "           -30.1250],\n",
      "         [-322.5000, -117.3125, -173.8750,  ..., -216.5000, -243.1250,\n",
      "          -322.0000],\n",
      "         ...,\n",
      "         [-378.2500, -141.1250, -184.1250,  ..., -234.2500, -265.5000,\n",
      "          -378.0000],\n",
      "         [  -4.4102,  -14.7109,  -51.8125,  ...,   -4.0430,   -0.5557,\n",
      "            -3.7812],\n",
      "         [ -24.9375,   -8.6328,  -48.2812,  ...,  -18.4219,  -13.7891,\n",
      "           -24.3125]]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([[0.0716, 0.9284]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Yes\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Hello\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "print(input_ids.input_ids)\n",
    "#print(model)\n",
    "tmp = model.get_input_embeddings().weight[input_ids.input_ids]\n",
    "tmp.to(device='cuda')\n",
    "\n",
    "conc = torch.cat((tmp,projected), dim=1).to(torch.float16)\n",
    "\n",
    "outputs = model(inputs_embeds=conc)\n",
    "noyes = [1294,3553]\n",
    "logits = outputs['logits']\n",
    "print(logits)\n",
    "logits = logits[:,1,noyes]\n",
    "\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "print(probs)\n",
    "predicted_token_id = torch.argmax(probs, dim=-1)\n",
    "\n",
    "if predicted_token_id.item() == 0:\n",
    "    predicted_token_id[0] = 1294\n",
    "else:\n",
    "    predicted_token_id[0] = 3553\n",
    "\n",
    "decoded_token = tokenizer.decode(predicted_token_id[0])\n",
    "print(decoded_token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, _, _, _ = data_split(df, pkl_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 2048])\n"
     ]
    }
   ],
   "source": [
    "proj = ProjectionNN()\n",
    "\n",
    "tmp12 = torch.tensor(train[0]).float()\n",
    "\n",
    "projected = proj(tmp12).to(device='cuda').to(torch.float16)\n",
    "print(projected.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n",
      "?\n",
      "\n",
      "Sure, here's a summary of the information I found about the topic:\n",
      "\n",
      "**Disclaimer:** I am not a medical professional and cannot provide medical advice. Please consult with a healthcare provider for any health concerns or questions.\n",
      "\n",
      "**Smoking can have both positive and negative effects on your health.**\n",
      "\n",
      "**Positive effects:**\n",
      "\n",
      "* **Reduced risk of heart disease:** Smoking can lower blood pressure, cholesterol levels, and other risk factors for heart disease.\n",
      "* **Reduced risk of stroke:** Smoking can reduce the risk of stroke by up to 50%.\n",
      "* **Lower risk of cancer:** Smoking can reduce the risk of certain types of cancer, such as lung cancer, mouth cancer, and throat cancer\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Is smoking good for you\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "test = model.get_input_embeddings().weight[input_ids.input_ids]\n",
    "#print(input_ids)\n",
    "\n",
    "outputs = model.generate(inputs_embeds=test, max_length = 150)\n",
    "\n",
    "notoken = torch.tensor(1294).to(device='cuda')\n",
    "yestoken = torch.tensor(3553).to(device='cuda')\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
