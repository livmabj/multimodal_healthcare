{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning of gemma-2b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL THE NECESSARY IMPORTS\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, HfArgumentParser, TrainingArguments\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "from peft import LoraConfig, TaskType, get_peft_model, get_peft_config\n",
    "\n",
    "# Filepath to embeddings\n",
    "fname = \"/mnt/mimic/data/HAIM/mimic_extras/embeddings.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the model\n",
    "\n",
    "Different versions, with huggingface LoRA-class or custom Adapter-module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efff97891ba3466db1cd89997c279dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LoRA parameter efficient fine-tuning\n",
    "# Parameters are freezed and small submodules with low-rank matrices ar inserted at the target layers.\n",
    "# initialization of model\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\", quantization_config=quantization_config,attn_implementation=\"sdpa\")\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,805,824 || all params: 2,515,978,240 || trainable%: 0.3897420034920493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GemmaForCausalLM(\n",
       "      (model): GemmaModel(\n",
       "        (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-17): 18 x GemmaDecoderLayer(\n",
       "            (self_attn): GemmaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): GemmaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): GemmaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=16384, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=16384, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=16384, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): GELUActivation()\n",
       "            )\n",
       "            (input_layernorm): GemmaRMSNorm()\n",
       "            (post_attention_layernorm): GemmaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): GemmaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model-structure and trainable parameters (this can be tuned by hyperparameters)\n",
    "model.print_trainable_parameters()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapter NN for parameter efficient fine-tuning\n",
    "# Adapters (bottleneck feed-forward networks) are added as modules to the layers of the model\n",
    "# adapting attention projections and MLP projections while freezing original model parameters\n",
    "\n",
    "class Adapter(nn.Module):\n",
    "    def __init__(self, size = 6, model_dim = 2048):\n",
    "        super().__init__()\n",
    "        self.adapter_block = nn.Sequential(\n",
    "            nn.Linear(model_dim, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, model_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output = self.adapter_block(x)\n",
    "        adapter_out = output + x\n",
    "\n",
    "        return adapter_out\n",
    "\n",
    "\n",
    "class Adaptered(nn.Module):\n",
    "    def __init__(self, orig_layer):\n",
    "        super().__init__()\n",
    "        self.orig_layer = orig_layer\n",
    "        self.adapter = Adapter()\n",
    "\n",
    "    def forward(self, *x):\n",
    "        orig_out = self.orig_layer(*x)\n",
    "        output = (self.adapter.forward(orig_out[0].unsqueeze(0))[0],)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "class model_with_adapter(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\", quantization_config=self.quantization_config,attn_implementation=\"sdpa\")\n",
    "        # Freeze the original model parameters\n",
    "        for params in self.model.parameters():\n",
    "            params.requires_grad = False\n",
    "        # Embed adapter layers into the transformer blocks \n",
    "        for i, gemma_layer in enumerate(self.model.model.layers):\n",
    "            gemma_layer.self_attn.q_proj = Adaptered(gemma_layer.self_attn.q_proj)\n",
    "            gemma_layer.self_attn.k_proj = Adaptered(gemma_layer.self_attn.k_proj)\n",
    "            gemma_layer.self_attn.v_proj = Adaptered(gemma_layer.self_attn.v_proj)\n",
    "            gemma_layer.self_attn.o_proj = Adaptered(gemma_layer.self_attn.o_proj)\n",
    "    \n",
    "            gemma_layer.mlp.gate_proj = Adaptered(gemma_layer.mlp.gate_proj)\n",
    "            gemma_layer.mlp.up_proj = Adaptered(gemma_layer.mlp.up_proj)\n",
    "            gemma_layer.mlp.down_proj = Adaptered(gemma_layer.mlp.down_proj)\n",
    "\n",
    "    def get_model(self):\n",
    "\n",
    "        return self.model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom get_parameters function\n",
    "def get_parameters(model):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_percentage = (trainable_params / total_params) * 100\n",
    "\n",
    "    trainable_params_str = \"{:,}\".format(trainable_params)\n",
    "    total_params_str = \"{:,}\".format(total_params)\n",
    "\n",
    "    print(f\"trainable params: {trainable_params_str} || all params: {total_params_str} || trainable%: {trainable_percentage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0198f130175d4f38a30e07dbba33415c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,355,380 || all params: 1,518,623,476 || trainable%: 0.2209487771674616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model_with_adapter(\n",
       "  (model): GemmaForCausalLM(\n",
       "    (model): GemmaModel(\n",
       "      (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-17): 18 x GemmaDecoderLayer(\n",
       "          (self_attn): GemmaSdpaAttention(\n",
       "            (q_proj): Adaptered(\n",
       "              (orig_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (adapter): Adapter(\n",
       "                (adapter_block): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=6, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=6, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (k_proj): Adaptered(\n",
       "              (orig_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (adapter): Adapter(\n",
       "                (adapter_block): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=6, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=6, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (v_proj): Adaptered(\n",
       "              (orig_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (adapter): Adapter(\n",
       "                (adapter_block): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=6, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=6, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (o_proj): Adaptered(\n",
       "              (orig_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (adapter): Adapter(\n",
       "                (adapter_block): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=6, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=6, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (rotary_emb): GemmaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): GemmaMLP(\n",
       "            (gate_proj): Adaptered(\n",
       "              (orig_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "              (adapter): Adapter(\n",
       "                (adapter_block): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=6, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=6, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (up_proj): Adaptered(\n",
       "              (orig_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "              (adapter): Adapter(\n",
       "                (adapter_block): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=6, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=6, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (down_proj): Adaptered(\n",
       "              (orig_layer): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
       "              (adapter): Adapter(\n",
       "                (adapter_block): Sequential(\n",
       "                  (0): Linear(in_features=2048, out_features=6, bias=True)\n",
       "                  (1): ReLU()\n",
       "                  (2): Linear(in_features=6, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (act_fn): GELUActivation()\n",
       "          )\n",
       "          (input_layernorm): GemmaRMSNorm()\n",
       "          (post_attention_layernorm): GemmaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): GemmaRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialization of adapter-model.\n",
    "# \n",
    "model = model_with_adapter().to('cuda')\n",
    "\n",
    "# Model-structure and trainable parameters (this can be tuned by hyperparameters)\n",
    "get_parameters(model)\n",
    "model\n",
    "\n",
    "# OBS A lot less params, not sure why.. (maybe cause of degeneration? or mistake, need to look into)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projection module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 1024\n",
    "projection_size = 6\n",
    "\n",
    "class ProjectionNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ProjectionNN, self).__init__()\n",
    "\n",
    "        # Architecture\n",
    "        self.fc1 = nn.Linear(embedding_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 2048 * projection_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(-1,6,2048)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching and preprocessing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_77726/4265014453.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_death_small48['y'] = 1\n",
      "/tmp/ipykernel_77726/4265014453.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_alive_big48['y'] = 0\n",
      "/tmp/ipykernel_77726/4265014453.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_death_big48['y'] = 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(fname)\n",
    "df_death_small48 = df[((df['img_length_of_stay'] < 48) & (df['death_status'] == 1))]\n",
    "df_alive_big48 = df[((df['img_length_of_stay'] >= 48) & (df['death_status'] == 0))]\n",
    "df_death_big48 = df[((df['img_length_of_stay'] >= 48) & (df['death_status'] == 1))]\n",
    "\n",
    "df_death_small48['y'] = 1\n",
    "df_alive_big48['y'] = 0\n",
    "df_death_big48['y'] = 0\n",
    "df = pd.concat([df_death_small48, df_alive_big48, df_death_big48], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     haim_id      vd_0      vd_1      vd_2  ...   vd_1021   vd_1022   vd_1023  y\n",
      "256     6557  0.005299  0.082119  0.274407  ...  0.000203  0.024739  0.005796  1\n",
      "259     6557  0.000000  0.079306  0.381579  ...  0.000000  0.022821  0.011584  1\n",
      "267     6558  0.005299  0.082119  0.274407  ...  0.000203  0.024739  0.005796  1\n",
      "270     6558  0.000000  0.079306  0.381579  ...  0.000000  0.022821  0.011584  1\n",
      "319     6581  0.002288  0.078941  0.088397  ...  0.000000  0.013369  0.168811  1\n",
      "\n",
      "[5 rows x 1026 columns]\n"
     ]
    }
   ],
   "source": [
    "vd_cols = df.filter(regex='^vd_')\n",
    "y_col = df[['y']]\n",
    "haim_col = df[['haim_id']]\n",
    "df = pd.concat([haim_col, vd_cols, y_col], axis=1)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(df, pkl_list):\n",
    "    train_id, test_id = train_test_split(pkl_list, test_size=0.3)\n",
    "    \n",
    "    train_idx = df[df['haim_id'].isin(train_id)]['haim_id'].tolist()\n",
    "    test_idx = df[df['haim_id'].isin(test_id)]['haim_id'].tolist()\n",
    "\n",
    "    x_train = df[df['haim_id'].isin(train_idx)].drop(['haim_id','y'],axis=1).values\n",
    "    x_test = df[df['haim_id'].isin(test_idx)].drop(['haim_id','y'],axis=1).values\n",
    "\n",
    "    y_train = df[df['haim_id'].isin(train_idx)]['y'].values\n",
    "    y_test = df[df['haim_id'].isin(test_idx)]['y'].values\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = torch.tensor(df.iloc[:, 1:1025].values, dtype=torch.float32)\n",
    "labels = df['y'].apply(lambda x: 'yes' if x == 0 else 'no').tolist()\n",
    "list_of_lists = df.iloc[:, 1:1025].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt function to be fed into training loop\n",
    "\n",
    "def formatting_func(example, emb, label):\n",
    "    text = f\"### INSTRUCTION: {'Use this input to create the correct label.'}\\n### INPUT: {emb}\\n### LABEL: {label}\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, embedding, labels): #removing formatting function\n",
    "        self.labels = labels\n",
    "        self.embedding = embedding\n",
    "        #self.formatting_func = formatting_func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        emb = self.embedding[idx]\n",
    "        sample = {\"Emb\": emb, \"Class\": label} #\"Func\": self.formatting_func\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Embeddings:\n",
      "tensor([[0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000, 0.9000,\n",
      "         1.0000],\n",
      "        [1.0000, 0.9000, 0.8000, 0.7000, 0.6000, 0.5000, 0.4000, 0.3000, 0.2000,\n",
      "         0.1000],\n",
      "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000],\n",
      "        [0.2000, 0.4000, 0.6000, 0.8000, 1.0000, 0.8000, 0.6000, 0.4000, 0.2000,\n",
      "         0.0000],\n",
      "        [0.9000, 0.8000, 0.7000, 0.6000, 0.5000, 0.4000, 0.3000, 0.2000, 0.1000,\n",
      "         0.0000]])\n",
      "\n",
      "Corresponding Labels:\n",
      "['yes', 'no', 'maybe', 'yes', 'no']\n"
     ]
    }
   ],
   "source": [
    "numerical_embeddings = torch.tensor([\n",
    "    [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    [1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "    [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n",
    "    [0.2, 0.4, 0.6, 0.8, 1.0, 0.8, 0.6, 0.4, 0.2, 0.0],\n",
    "    [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0]\n",
    "])\n",
    "\n",
    "# Corresponding labels\n",
    "labels = ['yes', 'no', 'maybe', 'yes', 'no']\n",
    "\n",
    "print(\"Numerical Embeddings:\")\n",
    "print(numerical_embeddings)\n",
    "print(\"\\nCorresponding Labels:\")\n",
    "print(labels)\n",
    "\n",
    "CD = CustomDataset(numerical_embeddings, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "     \n",
    "    emb_list, classes = [], []\n",
    "    for thing in batch:\n",
    "        #print(batch)\n",
    "        emb_list.append(thing['Emb'])\n",
    "        classes.append(tokenizer(thing['Class'], return_tensors=\"pt\"))\n",
    "    text = torch.tensor(emb_list)\n",
    "    classes = torch.cat([item['input_ids'] for item in classes], dim=0)\n",
    "    return text, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "CD = CustomDataset(list_of_lists, labels)\n",
    "\n",
    "DL_DS = DataLoader(CD, batch_size=2, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor([[5.2990e-03, 8.2119e-02, 2.7441e-01,  ..., 2.0308e-04, 2.4739e-02,\n",
      "         5.7963e-03],\n",
      "        [0.0000e+00, 7.9306e-02, 3.8158e-01,  ..., 0.0000e+00, 2.2821e-02,\n",
      "         1.1584e-02]]), tensor([[  2, 956],\n",
      "        [  2, 956]])), (tensor([[5.2990e-03, 8.2119e-02, 2.7441e-01,  ..., 2.0308e-04, 2.4739e-02,\n",
      "         5.7963e-03],\n",
      "        [0.0000e+00, 7.9306e-02, 3.8158e-01,  ..., 0.0000e+00, 2.2821e-02,\n",
      "         1.1584e-02]]), tensor([[  2, 956],\n",
      "        [  2, 956]])), (tensor([[5.2990e-03, 8.2119e-02, 2.7441e-01,  ..., 2.0308e-04, 2.4739e-02,\n",
      "         5.7963e-03],\n",
      "        [0.0000e+00, 7.9306e-02, 3.8158e-01,  ..., 0.0000e+00, 2.2821e-02,\n",
      "         1.1584e-02]]), tensor([[  2, 956],\n",
      "        [  2, 956]])), (tensor([[5.2990e-03, 8.2119e-02, 2.7441e-01,  ..., 2.0308e-04, 2.4739e-02,\n",
      "         5.7963e-03],\n",
      "        [0.0000e+00, 7.9306e-02, 3.8158e-01,  ..., 0.0000e+00, 2.2821e-02,\n",
      "         1.1584e-02]]), tensor([[  2, 956],\n",
      "        [  2, 956]])), (tensor([[5.2990e-03, 8.2119e-02, 2.7441e-01,  ..., 2.0308e-04, 2.4739e-02,\n",
      "         5.7963e-03],\n",
      "        [0.0000e+00, 7.9306e-02, 3.8158e-01,  ..., 0.0000e+00, 2.2821e-02,\n",
      "         1.1584e-02]]), tensor([[  2, 956],\n",
      "        [  2, 956]]))]\n"
     ]
    }
   ],
   "source": [
    "first_five_batches = [next(iter(DL_DS)) for _ in range(5)]\n",
    "print(first_five_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projection module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 1024\n",
    "projection_size = 6\n",
    "\n",
    "class ProjectionNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ProjectionNN, self).__init__()\n",
    "\n",
    "        # Architecture\n",
    "        self.fc1 = nn.Linear(embedding_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 2048 * projection_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(-1,6,2048)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, gemma, optimizer, loss_fn, train_loader, device):\n",
    "    # Train:\n",
    "    model.train()\n",
    "    train_loss_batches, train_acc_batches = [], []\n",
    "    num_batches = len(train_loader)\n",
    "    for batch_index, (x, y) in enumerate(train_loader, 1):\n",
    "        inputs, labels = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        embs = model.forward(inputs)\n",
    "        \n",
    "        \n",
    "\n",
    "        loss = loss_fn(z, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_batches.append(loss.item())\n",
    "\n",
    "        hard_preds = output_to_label(z)\n",
    "        acc_batch_avg = (hard_preds == labels).float().mean().item()\n",
    "        train_acc_batches.append(acc_batch_avg)\n",
    "\n",
    "    return model, train_loss_batches, train_acc_batches\n",
    "\n",
    "def validate(model, loss_fn, val_loader, device):\n",
    "    val_loss_cum = 0\n",
    "    val_acc_cum = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (x, y) in enumerate(val_loader, 1):\n",
    "            inputs, labels = x.to(device), y.to(device)\n",
    "            z = model.forward(inputs)\n",
    "\n",
    "            batch_loss = loss_fn(z, labels.float())\n",
    "            val_loss_cum += batch_loss.item()\n",
    "            hard_preds = output_to_label(z)\n",
    "            acc_batch_avg = (hard_preds == labels).float().mean().item()\n",
    "            val_acc_cum += acc_batch_avg\n",
    "    return val_loss_cum/len(val_loader), val_acc_cum/len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, optimizer, loss_fn, train_loader, val_loader, num_epochs, print_every):\n",
    "    print(\"Starting training\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model, train_loss, train_acc = train_epoch(model,\n",
    "                                                   optimizer,\n",
    "                                                   loss_fn,\n",
    "                                                   train_loader,\n",
    "                                                   val_loader,\n",
    "                                                   device,\n",
    "                                                   print_every)\n",
    "        val_loss, val_acc = validate(model, loss_fn, val_loader, device)\n",
    "        print(f\"Epoch {epoch}/{num_epochs}: \"\n",
    "              f\"Train loss: {sum(train_loss)/len(train_loss):.3f}, \"\n",
    "              f\"Train acc.: {sum(train_acc)/len(train_acc):.3f}, \"\n",
    "              f\"Val. loss: {val_loss:.3f}, \"\n",
    "              f\"Val. acc.: {val_acc:.3f}\")\n",
    "        train_losses.extend(train_loss)\n",
    "        train_accs.extend(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "    return model, train_losses, train_accs, val_losses, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Hello, can you tell me what the input_embs are saying.，,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Hello, can you tell me what the input_embs are saying.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "#print(input_ids)\n",
    "#print(next(model.children()))\n",
    "tmp = model(**input_ids, output_hidden_states=True).hidden_states[-1]\n",
    "\n",
    "outputs = model.generate(**input_ids, inputs_embeds=tmp, max_length=50)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3562, -0.8369, -0.2644,  ..., -0.4573, -0.5049, -0.4429],\n",
      "         [-1.0010, -1.4258, -1.4482,  ..., -0.8257, -0.2101, -1.2383],\n",
      "         [-0.7925,  0.6221, -1.1826,  ..., -0.4849,  0.8032, -1.1758],\n",
      "         ...,\n",
      "         [-0.2217, -0.6143,  0.0714,  ...,  2.7500, -1.4404, -1.5752],\n",
      "         [ 0.4551, -1.8252,  0.0773,  ...,  1.9189, -0.7290, -1.5742],\n",
      "         [ 0.7378,  0.9209,  0.1779,  ...,  0.3516, -0.0546, -0.5752]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<MulBackward0>)\n",
      "[924, 1959, 1959, 1959, 1959, 1959, 1959, 1959, 1959, 1959, 1645, 981, 1790, 1959, 1959, 1959]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ieslesslesslesslesslesslesslesslesslessuserнаinesslesslessless'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conerting hidden to tokens (not working great)\n",
    "\n",
    "print(tmp)\n",
    "\n",
    "token_ids = torch.argmax(tmp, dim=-1)\n",
    "\n",
    "decoded_tokens = tokenizer.convert_ids_to_tokens(token_ids[0].tolist())\n",
    "converted_token_ids = tokenizer.convert_tokens_to_ids(decoded_tokens)\n",
    "print(converted_token_ids)\n",
    "tokenizer.decode(converted_token_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.2 ('gemma')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10601c873fb2576e1e1a48994b394f584387b3d54a28d8ac07023c991446672f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
