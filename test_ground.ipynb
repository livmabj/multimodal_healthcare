{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from focal_loss.focal_loss import FocalLoss\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, WeightedRandomSampler, RandomSampler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from big_utils import *\n",
    "\n",
    "# Filepath to embeddings\n",
    "fname = '/mnt/mimic/data/HAIM/mimic_extras/embeddings.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d95e4e9d2ba8406381b24f630721b69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, \n",
    "                                         bnb_4bit_use_double_quant=True,\n",
    "                                         bnb_4bit_quant_type=\"nf4\",\n",
    "                                         bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=\"auto\", quantization_config=quantization_config)\n",
    "\n",
    "# Read data & extract labels and features\n",
    "df = pd.read_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liv/multimodal_healthcare/big_utils.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_death_small48['y'] = 1\n",
      "/home/liv/multimodal_healthcare/big_utils.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_alive_big48['y'] = 0\n",
      "/home/liv/multimodal_healthcare/big_utils.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_death_big48['y'] = 0\n",
      "/home/liv/multimodal_healthcare/big_utils.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_alive_small48['y'] = 0\n",
      "/home/liv/multimodal_healthcare/big_utils.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_alive_small48['y'] = 1\n",
      "/home/liv/multimodal_healthcare/big_utils.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_alive_big48['y'] = 0\n",
      "/home/liv/multimodal_healthcare/big_utils.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_death['y'] = 0\n"
     ]
    }
   ],
   "source": [
    "# Load train/val sets and create data loaders\n",
    "batch_size = 8\n",
    "\n",
    "Data = DataSplit(df)\n",
    "Data.split_data('all')\n",
    "\n",
    "X, V = Data.get_data()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "train_set = CustomDataset(X.values.tolist(), Data.y_train.tolist())\n",
    "val_set = CustomDataset(V.values.tolist(), Data.y_val.tolist())\n",
    "\n",
    "transposed_Y = list(map(list, zip(*Data.y_train.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan, nan, nan, nan, 1.0, nan, nan, nan, 1.0, 1.0, 0, 0], [nan, nan, nan, nan, nan, nan, nan, 0.0, 1.0, 1.0, 0, 0], [nan, nan, nan, nan, nan, 1.0, nan, 0.0, 1.0, nan, 0, 0], [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0, 0], [nan, nan, nan, nan, nan, 1.0, nan, 0.0, 1.0, nan, 0, 0], [nan, nan, nan, nan, nan, nan, nan, 0.0, nan, nan, 0, 0], [nan, nan, nan, nan, nan, 1.0, nan, 0.0, -1.0, 0.0, 0, 0], [nan, nan, nan, nan, nan, 1.0, nan, 0.0, 0.0, 1.0, 0, 0], [nan, nan, nan, nan, 0.0, nan, 1.0, nan, 1.0, -1.0, 0, 0], [nan, nan, -1.0, nan, nan, 1.0, 1.0, 0.0, nan, nan, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(Data.y_train.tolist()[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan, nan, nan, nan, nan, -1.0]\n",
      "[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.0, nan, nan, nan, nan, nan, nan, nan, 0.0, nan]\n",
      "[nan, nan, 1.0, nan, 1.0, nan, 1.0, 1.0, nan, 1.0]\n",
      "[nan, nan, nan, nan, nan, nan, nan, nan, 1.0, 1.0]\n",
      "[nan, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, nan, 0.0]\n",
      "[1.0, 1.0, 1.0, nan, 1.0, nan, -1.0, 0.0, 1.0, nan]\n",
      "[1.0, 1.0, nan, nan, nan, nan, 0.0, 1.0, -1.0, nan]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "for y in transposed_Y:\n",
    "    print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 10.3571,  29.7640, 566.4758], device='cuda:0'), tensor([  9.1859,  30.1991, 182.9245], device='cuda:0'), tensor([3.7322, 9.3533, 6.3534], device='cuda:0'), tensor([ 3.4092,  5.7080, 19.2552], device='cuda:0'), tensor([1.6664, 5.5757, 6.3904], device='cuda:0'), tensor([20.9051,  1.5318,  9.9890], device='cuda:0'), tensor([13.8496,  1.6084, 24.7509], device='cuda:0'), tensor([ 0.6262,  7.2866, 54.1163], device='cuda:0'), tensor([1.6278, 2.3627, 6.4884], device='cuda:0'), tensor([ 2.7703,  1.6306, 11.3699], device='cuda:0'), tensor([0.5517, 5.3336], device='cuda:0'), tensor([ 0.5127, 20.1385], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "weight_per_class = []\n",
    "for y in transposed_Y[:-2]:\n",
    "    y = torch.tensor(y)\n",
    "    mask = ~torch.isnan(y)\n",
    "    w0 = len(y[mask])/(2*sum(y[mask] == 0))\n",
    "    w1 = len(y)/(2*sum(y == 1))\n",
    "    w2 = len(y)/(2*sum(y == -1))\n",
    "    weight_per_class.append(torch.tensor([w0, w1, w2], dtype = torch.float).to(\"cuda\"))\n",
    "\n",
    "for y in transposed_Y[-2:]:\n",
    "    y = torch.tensor(y)\n",
    "    w0 = len(y)/(2*sum(y == 0))\n",
    "    w1 = len(y)/(2*sum(y == 1))\n",
    "    weight_per_class.append(torch.tensor([w0, w1], dtype = torch.float).to(\"cuda\"))\n",
    "print(weight_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = RandomSampler(train_set, replacement=False)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, sampler=sampler, num_workers=5)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting model and hyperparameters\n",
    "vd_model = AutoEncoder(1024,2048)\n",
    "ts_model = AutoEncoder(451,2048)\n",
    "n_rad_model = AutoEncoder(768,2048)\n",
    "vd_optimizer = optim.Adam(vd_model.parameters(), lr=0.0005, weight_decay=0.0003)\n",
    "ts_optimizer = optim.Adam(ts_model.parameters(), lr=0.0005, weight_decay=0.0003)\n",
    "n_rad_optimizer = optim.Adam(n_rad_model.parameters(), lr=0.003, weight_decay=0.003)\n",
    "optimizers = [vd_optimizer, ts_optimizer, n_rad_optimizer]\n",
    "#optimizers = [n_rad_optimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mse = nn.MSELoss()\n",
    "loss_fns = []\n",
    "for weight in weight_per_class:\n",
    "    loss_fns.append(nn.CrossEntropyLoss(weight=weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoEncoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(vd_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 768])\n",
      "tensor([[0.0074, 0.0000, 0.0000,  ..., 0.0000, 0.0271, 0.1658],\n",
      "        [0.0720, 0.0000, 0.1096,  ..., 0.0000, 0.1047, 0.0000],\n",
      "        [0.1739, 0.0940, 0.0000,  ..., 0.0000, 0.0793, 0.0000],\n",
      "        ...,\n",
      "        [0.1107, 0.1327, 0.0000,  ..., 0.0010, 0.0852, 0.0000],\n",
      "        [0.1733, 0.0655, 0.0000,  ..., 0.0934, 0.1671, 0.0352],\n",
      "        [0.1070, 0.0841, 0.0000,  ..., 0.0000, 0.0449, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<ReluBackward0>)\n",
      "torch.Size([8, 768])\n",
      "tensor([[0.0503, 0.1842, 0.0275,  ..., 0.0553, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0010, 0.0133],\n",
      "        [0.0000, 0.1979, 0.2553,  ..., 0.0958, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0886, 0.0704, 0.0000,  ..., 0.0000, 0.0263, 0.0000],\n",
      "        [0.2032, 0.0000, 0.1534,  ..., 0.1251, 0.0459, 0.0735],\n",
      "        [0.0616, 0.0000, 0.0000,  ..., 0.0000, 0.2095, 0.2468]],\n",
      "       device='cuda:0', grad_fn=<ReluBackward0>)\n",
      "torch.Size([8, 768])\n",
      "tensor([[0.1316, 0.0506, 0.0418,  ..., 0.0000, 0.0000, 0.0110],\n",
      "        [0.1004, 0.0515, 0.0160,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1036, 0.0984],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1291],\n",
      "        [0.1377, 0.0000, 0.0000,  ..., 0.0000, 0.0997, 0.1905],\n",
      "        [0.0170, 0.0000, 0.0787,  ..., 0.0653, 0.0772, 0.0676]],\n",
      "       device='cuda:0', grad_fn=<ReluBackward0>)\n",
      "orig tensor(True, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "n_rad_model.to('cuda')\n",
    "for batch_index, (x, y) in enumerate(train_loader, 1):\n",
    "        #print('stuck')\n",
    "        inputs, labels = x, y.to('cuda')\n",
    "        vd_inputs = inputs['n_rad'].to('cuda')\n",
    "        has_nan = torch.isnan(vd_inputs).any()\n",
    "        if has_nan:\n",
    "                print('orig', has_nan)\n",
    "                break\n",
    "        print(vd_inputs.shape)\n",
    "        enc = n_rad_model.encoder(vd_inputs)\n",
    "        has_nan = torch.isnan(enc).any()\n",
    "        if has_nan:\n",
    "                print('enc',has_nan)\n",
    "                break\n",
    "        print(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "torch.Size([8, 768])\n",
      "torch.Size([8, 768])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      2\u001b[0m beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m----> 4\u001b[0m fine_tuned_vd, fine_tuned_ts, fine_tuned_n_rad, train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvd_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rad_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_mse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgemma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/multimodal_healthcare/big_utils.py:377\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(vd_model, ts_model, n_rad_model, optimizers, mse_loss, loss_fns, train_loader, val_loader, num_epochs, gemma, beta)\u001b[0m\n\u001b[1;32m    374\u001b[0m best_f1 \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    376\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m--> 377\u001b[0m     vd_model, ts_model, n_rad_model, train_loss \u001b[39m=\u001b[39m train_epoch(vd_model,\n\u001b[1;32m    378\u001b[0m                                     ts_model,\n\u001b[1;32m    379\u001b[0m                                     n_rad_model,\n\u001b[1;32m    380\u001b[0m                                     optimizers,\n\u001b[1;32m    381\u001b[0m                                     mse_loss,\n\u001b[1;32m    382\u001b[0m                                     loss_fns,\n\u001b[1;32m    383\u001b[0m                                     train_loader,\n\u001b[1;32m    384\u001b[0m                                     device,\n\u001b[1;32m    385\u001b[0m                                     gemma,\n\u001b[1;32m    386\u001b[0m                                     beta)\n\u001b[1;32m    387\u001b[0m     val_loss, preds, labels \u001b[39m=\u001b[39m validate(vd_model, ts_model, n_rad_model, mse_loss, loss_fns, val_loader, device, gemma, beta)\n\u001b[1;32m    389\u001b[0m     preds_arrays \u001b[39m=\u001b[39m [t\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy() \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m preds]\n",
      "File \u001b[0;32m~/multimodal_healthcare/big_utils.py:308\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(vd_model, ts_model, n_rad_model, optimizers, mse_loss, loss_fns, train_loader, device, gemma, beta)\u001b[0m\n\u001b[1;32m    305\u001b[0m loss_mse \u001b[39m=\u001b[39m custom_mse_loss(all_decoded, inputs, mse_loss, device)\n\u001b[1;32m    306\u001b[0m loss \u001b[39m=\u001b[39m loss_bce \u001b[39m+\u001b[39m beta\u001b[39m*\u001b[39mloss_mse\n\u001b[0;32m--> 308\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    309\u001b[0m \u001b[39m#for optimizer in optimizers:\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[39m#    optimizer.step()\u001b[39;00m\n\u001b[1;32m    311\u001b[0m optimizers[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mstep\n",
      "File \u001b[0;32m/home/edgelab/miniconda3/envs/gemma/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m/home/edgelab/miniconda3/envs/gemma/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "beta = 0.1\n",
    "\n",
    "fine_tuned_vd, fine_tuned_ts, fine_tuned_n_rad, train_losses, val_losses = training_loop(vd_model, ts_model, n_rad_model, optimizers, loss_mse, loss_fns, train_loader, val_loader, num_epochs, gemma, beta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.2 ('gemma')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10601c873fb2576e1e1a48994b394f584387b3d54a28d8ac07023c991446672f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
